# SelfQueryRetriever 실험


이 문서는 LangChain 라이브러리의 **SelfQueryRetriever** 기능을 활용한 검색 시스템 예제 노트북 (`07_2.self_query_retriever.ipynb`)의 코드를 자세히 설명합니다. 각 코드 셀과 그 출력 결과를 순서대로 다루며, 코드의 목적, 동작 원리, 사용된 라이브러리 함수와 인자의 의미를 학부 수준에서 이해할 수 있도록 친절히 설명합니다. 또한, 출력된 결과가 의미하는 바와 검색 성능 지표(정밀도, 재현율, MRR, MAP 등)의 의미를 해석합니다.

## 코드 셀 1 설명: 필요한 패키지 설치 (의존성 설정)

이 코드 셀은 검색 실험에 필요한 파이썬 패키지를 설치합니다. 주로 `%pip install` 매직 명령을 사용하여 아래와 같은 라이브러리를 최신 버전으로 업그레이드하거나 설치합니다:

* **python-dotenv**: `.env` 파일에서 환경변수를 불러오는 라이브러리입니다. OpenAI API 키나 Pinecone API 키 등을 .env에 저장해두고 불러오는 데 사용합니다.
* **pandas**: 데이터 처리를 위한 라이브러리입니다. CSV 파일로 된 쿼리 데이터(`queries_meta.csv`)를 불러오고 다루기 위해 사용합니다.
* **pinecone**: Pinecone 벡터 데이터베이스에 접속하고 조작하기 위한 SDK입니다. Pinecone은 클라우드 기반의 벡터 스토어로, 문서 임베딩을 저장하고 유사도 검색을 수행합니다.
* **langchain** 및 관련 패키지:

  * *langchain-openai*: LangChain에서 OpenAI의 API(예: GPT-3/4, 임베딩 모델 등)를 쉽게 사용하기 위한 모듈입니다.
  * *langchain-pinecone*: LangChain에서 Pinecone 벡터 DB를 연동하기 위한 모듈입니다.
  * *langchain-community*: LangChain 커뮤니티에서 제공하는 유용한 구성 요소를 포함한 모듈로 보입니다 (여기서는 SelfQueryRetriever 등 특별 기능에 활용될 수 있습니다).
* **lark**: 파서를 생성하는 라이브러리입니다. SelfQueryRetriever가 LLM의 출력(예: 필터 조건)을 구문 분석할 때 사용되는 문법을 정의하는 데 활용됩니다.
* **scikit-learn**: 머신러닝 및 데이터 분석용 라이브러리입니다. 여기서는 직접 사용되지 않을 수도 있지만, 예를 들어 벡터 연산이나 평가 지표 계산에 활용 가능성이 있습니다.
* **matplotlib**: 데이터 시각화를 위한 라이브러리입니다. 검색 성능 비교를 그래프로 그려서 보여주는 데 사용됩니다.

위 명령을 실행하면 해당 패키지들이 설치되거나 이미 설치되어 있으면 *Requirement already satisfied* 메시지가 출력됩니다. `%pip install -U` 옵션은 패키지를 업그레이드(-U)까지 시도하는 것인데, 현재 노트북 환경에 이미 최신 버전이 설치되어 있음을 알 수 있습니다.

```python
%pip install -U python-dotenv pandas pinecone langchain langchain-openai langchain-pinecone langchain-community lark scikit-learn matplotlib
```

## 출력 결과 1 해석: 패키지 설치 로그

코드를 실행하면 터미널 형태로 각 패키지의 설치 로그가 출력됩니다. 예를 들어:

```
Requirement already satisfied: python-dotenv ...
Requirement already satisfied: pandas ...
...
Requirement already satisfied: matplotlib ...
Note: you may need to restart the kernel to use updated packages.
```

"Requirement already satisfied"라는 메시지는 해당 패키지가 이미 환경에 설치되어 최신 상태임을 나타냅니다. 따라서 추가 설치가 불필요하며, 환경이 이미 준비되어 있다는 의미입니다. 마지막에 \*"Note: you may need to restart the kernel to use updated packages."\*라는 문구는 일반적인 pip 경고로, 패키지를 업데이트한 경우 커널 재시작이 필요할 수 있다는 안내입니다. 이 경우 대부분의 패키지가 이미 설치되어 있으므로 바로 다음 단계로 진행할 수 있습니다.

---

## 코드 셀 2 설명: 환경 변수 로드 및 API 키 설정

이 코드 셀에서는 OpenAI와 Pinecone 서비스를 사용하기 위한 **환경 변수**들을 로드합니다. `python-dotenv` 패키지의 `load_dotenv()` 함수를 사용하여, 현재 작업 경로의 `.env` 파일에 저장된 키들을 불러옵니다. 그런 다음 `os.getenv()`를 통해 필요한 API 키와 설정 값을 가져와 파이썬 변수에 저장합니다:

* `OPENAI_API_KEY`: OpenAI API를 사용하기 위한 키 (예: GPT-4, 임베딩 모델 호출에 필요).
* `OPENAI_LLM_MODEL`: LangChain에서 사용할 대형 언어 모델(LLM)의 이름. 예를 들어 사용자는 `.env`에 `OPENAI_LLM_MODEL="gpt-4"` 혹은 `"gpt-3.5-turbo"` 등을 지정해 놓았을 수 있습니다. 여기서는 \*'gpt-4o-mini'\*처럼 커스텀/프록시 모델 이름 예시를 주석으로 달아두었습니다.
* `OPENAI_EMBEDDING_MODEL`: 문서 임베딩에 사용할 OpenAI 임베딩 모델의 이름입니다. 예를 들어 `'text-embedding-ada-002'`와 같은 최신 임베딩 모델 이름을 .env에 지정해 둘 수 있습니다 (예시로 *'text-embedding-3-small'* 등 언급).
* `PINECONE_API_KEY`: Pinecone 벡터 DB 서비스에 접속하기 위한 API 키.
* `PINECONE_INDEX_REGION` 및 `PINECONE_INDEX_CLOUD`: Pinecone 인덱스가 호스팅된 리전 정보와 클라우드 정보. Pinecone에서 인덱스를 생성할 때 부여되는 지역 (예: "asia-northeast1-gcp")과 클라우드 제공자 정보입니다.
* `PINECONE_INDEX_NAME`: Pinecone에서 미리 생성해둔 인덱스의 이름입니다 (예시: `'ir'`). 벡터 데이터베이스에서 우리가 사용할 컬렉션의 식별자입니다.
* `PINECONE_INDEX_METRIC`: Pinecone 인덱스에서 벡터 유사도를 계산할 때 사용하는 메트릭(예: `"cosine"`, `"dotproduct"`, `"euclidean"` 등).
* `PINECONE_INDEX_DIMENSION`: 벡터의 차원 수 (정수). 임베딩 모델에 따라 차원이 정해지는데, 해당 임베딩 벡터의 크기를 지정합니다.

환경 변수를 모두 불러온 후 `print("환경 변수 로딩 완료")`를 통해 성공적으로 로드되었음을 알립니다.

```python
import os
from dotenv import load_dotenv

load_dotenv()  # .env 파일에서 환경 변수 불러오기

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_LLM_MODEL = os.getenv("OPENAI_LLM_MODEL")        # 예: 'gpt-4'
OPENAI_EMBEDDING_MODEL = os.getenv("OPENAI_EMBEDDING_MODEL")  # 예: 'text-embedding-ada-002'
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_REGION = os.getenv("PINECONE_INDEX_REGION")
PINECONE_INDEX_CLOUD = os.getenv("PINECONE_INDEX_CLOUD")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME")   # 예: 'ir'
PINECONE_INDEX_METRIC = os.getenv("PINECONE_INDEX_METRIC")
PINECONE_INDEX_DIMENSION = int(os.getenv("PINECONE_INDEX_DIMENSION"))

print("환경 변수 로딩 완료")
```

## 출력 결과 2 해석: 환경 변수 로딩 확인

```
환경 변수 로딩 완료
```

위와 같이 출력되면, .env 파일에 저장된 API 키와 설정 값들이 성공적으로 메모리에 불러와졌다는 뜻입니다. 이후 단계에서 OpenAI의 LLM과 임베딩 모델, Pinecone 벡터 스토어에 접속할 때 이 값들이 사용됩니다. 만약 `.env` 파일이 없거나 값이 비어 있다면 `None`이 할당되어 에러가 발생할 수 있지만, 해당 메시지가 떴다는 것은 필요한 환경 설정이 완료되었음을 의미합니다.

---

## 코드 셀 3 설명: 쿼리 데이터 로드 및 확인

이 코드 셀에서는 검색 실험에 사용할 **질의(Query) 데이터**를 불러옵니다. `pandas`를 이용해 CSV 파일 `queries_meta.csv`를 읽어 데이터프레임 `queries_df`로 저장합니다. 이 파일에는 여러 사용자의 질의와 그에 대한 \*\*정답 문서 레이블(메타데이터)\*\*이 들어 있습니다. 코드는 다음을 수행합니다:

* `pd.read_csv("queries_meta.csv")`: 현재 디렉토리에 있는 `queries_meta.csv` 파일을 읽어들여 `queries_df`라는 `DataFrame` 객체로 만듭니다. 이 데이터프레임은 여러 행(row)을 가지며, 각 행은 하나의 질의를 나타냅니다. 주요 열(column)로 **질의 ID**, **질의 내용**, **관련 문서 IDs** 등이 포함되어 있을 것입니다.
* `print(f"질의 수: {len(queries_df)}")`: 데이터프레임의 행 수를 출력하여 총 질의 개수를 확인합니다.
* `queries_df.head()`: 데이터프레임의 **앞부분 5개 행**을 출력합니다. 이를 통해 데이터가 제대로 로드되었는지, 어떤 형식으로 되어 있는지 표로 확인할 수 있습니다.

```python
import pandas as pd

# queries_meta.csv 로드
queries_df = pd.read_csv("queries_meta.csv")
print(f"질의 수: {len(queries_df)}")
queries_df.head()
```

## 출력 결과 3 해석: 질의 데이터 내용 및 형식

```
질의 수: 30
     query_id                 query_text                       relevant_doc_ids
0      Q01      저자 김지훈의 문서를 모두 보여줘        D1=1;D11=1;D21=1
1      Q02      저자 이지혜의 문서를 모두 보여줘        D2=1;D12=1;D22=1
2      Q03      저자 박지훈의 문서를 모두 보여줘        D3=1;D13=1;D23=1
3      Q04      저자 최수정의 문서를 모두 보여줘        D4=1;D14=1;D24=1
4      Q05      저자 정우성의 문서를 모두 보여줘        D5=1;D15=1;D25=1
...   ...                     ...                        ...
```

* **질의 수: 30** – 총 30개의 질의가 데이터에 포함되어 있음을 보여줍니다. 따라서 30개의 검색 실험을 진행할 것입니다.
* 데이터프레임 출력 표에서는 각 행에 대해 `query_id`, `query_text`, `relevant_doc_ids` 컬럼이 보입니다. 예를 들어 첫 번째 행(`Q01`)은:

  * `query_text`: **"저자 김지훈의 문서를 모두 보여줘"** – 사용자 질의 문장입니다. 한국어로 "저자 김지훈의 문서를 모두 보여줘"라는 요청으로, **"김지훈"이라는 저자가 작성한 문서들을 모두 달라**는 의미의 질의입니다.
  * `relevant_doc_ids`: **"D1=1;D11=1;D21=1"** – 해당 질의에 대한 **정답 문서들의 ID 목록**입니다. 여기서 'D1', 'D11', 'D21' 등이 문서 ID를 나타내고 '=1'은 해당 문서가 이 질의에 \*\*관련됨(relevant)\*\*을 의미합니다. 여러 문서가 세미콜론(`;`)으로 구분되어 있는 것으로 보아, 이 질의에 해당하는 관련 문서들이 D1, D11, D21 총 3개임을 알 수 있습니다. (모든 쿼리에 대해 각 저자가 3개의 관련 문서를 가지고 있는 것처럼 보입니다.)
* 이러한 형식으로 30개의 질의가 들어 있으며, 대부분 **"저자 OOO의 문서를 모두 보여줘"** 형태를 띠고 있습니다. 각 질의마다 해당 저자가 작성한 문서 3개가 정답으로 레이블되어 있습니다. 이 데이터는 이후 검색 결과와 비교하여 검색 알고리즘의 성능(얼마나 관련 문서를 잘 찾아내는지)을 평가하는 데 사용됩니다.

---

## 코드 셀 4 설명: Pinecone 벡터 스토어 설정 (Dense 임베딩 검색 준비)

이 코드 셀에서는 \*\*벡터 데이터베이스(Vector DB)\*\*인 Pinecone에 연결하고, OpenAI 임베딩 모델을 이용한 **벡터 스토어**를 설정합니다. 이는 "Dense Retrieval"이라 불리는 방식을 구현하기 위함입니다. Dense Retrieval란 **문서 내용**을 **수치 임베딩 벡터**로 표현해 놓고, 질의 역시 임베딩으로 변환한 뒤 벡터 공간 상의 **유사도 검색**으로 관련 문서를 찾는 방법입니다.

구체적으로 코드가 하는 일은 다음과 같습니다:

* `Pinecone(api_key=...)`를 호출하여 Pinecone 서비스에 **클라이언트 인스턴스**(`pc`)를 생성합니다. `PINECONE_API_KEY`를 이용해 인증하며, 이 객체를 통해 Pinecone 인덱스들에 접근할 수 있습니다.
* `pc.Index(PINECONE_INDEX_NAME)`를 통해 **특정 인덱스에 연결**합니다. `PINECONE_INDEX_NAME`은 이전에 환경 변수로 불러온 인덱스 이름(예: "ir")이며, 해당 이름의 벡터 인덱스가 Pinecone에 미리 생성되어 있어야 합니다. 이 줄 실행 후 `index` 객체를 통해 그 벡터 DB에 질의하거나 삽입할 수 있게 됩니다.
* `OpenAIEmbeddings(model=..., openai_api_key=...)`를 호출하여 **임베딩 모델**을 초기화합니다. LangChain의 `OpenAIEmbeddings` 클래스는 지정한 OpenAI 임베딩 모델(`OPENAI_EMBEDDING_MODEL`)을 사용해 텍스트를 벡터로 바꿔주는 기능을 합니다. `openai_api_key`를 제공하여 OpenAI API 호출을 인증합니다.

  * 예를 들어, `OPENAI_EMBEDDING_MODEL`이 `"text-embedding-ada-002"`라면 이 모델을 사용해 문서와 질의를 1536차원의 임베딩으로 변환하게 될 것입니다.
* `PineconeVectorStore(index_name=..., embedding=...)`를 생성하여 **벡터 스토어** 객체를 만듭니다. LangChain의 `PineconeVectorStore`는 Pinecone 인덱스를 Python 객체로 추상화한 것입니다. `index_name`으로 Pinecone 인덱스를 지정하고, `embedding`으로 앞서 만든 임베딩 모델 객체를 넘겨주었습니다. 이 벡터 스토어는 **문서의 임베딩을 Pinecone에 저장하고**, **유사도 검색**을 간편하게 해주는 인터페이스를 제공합니다.

  * (내부적으로 이 객체는 Pinecone 인덱스에 연결되어 있어 `vector_store.similarity_search(query, k)` 등 메서드를 호출하면 자동으로 텍스트 `query`를 임베딩하고 Pinecone에서 k개의 유사한 벡터를 검색하는 기능을 수행하게 됩니다.)
* 마지막으로 `print("Pinecone 및 Dense Retrieval 설정 완료")`를 출력하여 Pinecone 연결 및 벡터 스토어 구성이 끝났음을 알려줍니다.

```python
from pinecone import Pinecone
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

# Pinecone 클라이언트 연결
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(PINECONE_INDEX_NAME)

# 임베딩 모델 생성 (OpenAI의 embedding 모델 사용)
embedding_model = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL, openai_api_key=OPENAI_API_KEY)

# Dense 벡터 스토어 설정 (Pinecone 인덱스 + 임베딩 결합)
vector_store = PineconeVectorStore(index_name=PINECONE_INDEX_NAME, embedding=embedding_model)

print("Pinecone 및 Dense Retrieval 설정 완료")
```

## 출력 결과 4 해석: 벡터 스토어 설정 완료 메시지

```
Pinecone 및 Dense Retrieval 설정 완료
```

이 메시지가 출력되면, 다음이 정상적으로 이루어진 것입니다:

* Pinecone 서비스에 API 키를 통해 연결 성공.
* 지정한 이름의 Pinecone 인덱스를 가져와 접근 가능.
* OpenAI 임베딩 모델 객체 생성 성공 (API 키 유효).
* PineconeVectorStore 객체 생성 성공.

이로써 **Dense Retrieval**을 위한 준비가 끝났습니다. 즉, 이제 주어진 질의 문장을 임베딩 벡터로 변환한 후 Pinecone 인덱스에서 유사도가 가장 높은 문서 벡터들을 가져올 수 있는 상태입니다. 이후 코드는 이 `vector_store` 객체를 사용하여 실제 **질의에 대한 벡터 검색**을 수행하게 됩니다.

---

## 코드 셀 5 설명: SelfQueryRetriever 설정 (LLM 기반 질의 변환기 구성)

이 코드 셀에서는 **Self-Query Retriever**를 초기화합니다. SelfQueryRetriever는 LangChain의 고급 기능으로, \*\*대형 언어 모델(LLM)\*\*을 활용하여 사용자의 자연어 질의를 **벡터 검색에 적합한 질의 및 메타데이터 필터**로 변환해주는 특별한 retriever입니다. 이를 통해 단순한 벡터 유사도 검색보다 더 정확한 검색을 할 수 있습니다.

코드 셀에서 수행하는 작업은 다음과 같습니다:

* `ChatOpenAI(model_name=..., openai_api_key=..., temperature=0.0)`: OpenAI의 채팅 모델(예: GPT-4 또는 GPT-3.5 시리즈)을 사용하기 위한 LangChain LLM 객체를 생성합니다.

  * `model_name`에는 사용할 LLM 모델의 이름이 들어갑니다 (`OPENAI_LLM_MODEL`, .env에서 불러온 값).
  * `temperature=0.0`은 **생성의 무작위성**을 0으로 설정한 것으로, 항상 동일하고 일관된 출력을 얻기 위해서입니다. (이 경우 LLM은 확률적으로 가장 높은 답변만을 선택하여 변동 없이 출력하게 됩니다. 이는 질의->필터 변환을 신뢰성 있게 하기 위해 중요합니다.)
  * 이 `chat_model` 객체는 뒤에서 SelfQueryRetriever가 질의를 이해하고 필터링 조건을 생성하는 데 사용됩니다.
* **메타데이터 필드 정보 정의**: `AttributeInfo` 클래스를 이용해 벡터 DB의 문서들이 가지는 메타데이터 속성들을 LLM에게 설명합니다. `metadata_field_info` 리스트에는 다음과 같은 항목들이 정의됩니다:

  * `AttributeInfo(name='author', type='string', description='문서를 작성한 저자 이름')`: 문서의 **저자(author)** 정보. 문자열로 취급되며, LLM에게 "이 필드는 문서를 작성한 저자의 이름을 담고 있다"고 설명합니다.
  * `AttributeInfo(name='category', type='list[string]', description='문서의 분류 리스트')`: 문서의 **분류(category)** 정보. 하나의 문서가 여러 주제 분류에 속할 수 있어 리스트 형식의 문자열로 저장되어 있다고 설명합니다.
  * `AttributeInfo(name='text', type='string', description='문서 본문 요약 또는 대표 텍스트')`: 문서의 \*\*본문 내용(text)\*\*에 해당하는 정보. 여기서는 본문 전체가 아닌 요약이나 대표 텍스트가 저장되어 있을 수 있다고 가정합니다.

  이처럼 메타데이터 필드 정보를 설정하는 이유는, LLM이 사용자의 자연어 질문에서 **특정 필드에 대한 조건**을 이해하도록 돕기 위해서입니다. 예를 들어 "저자 김지훈의 문서"라는 질문이 들어오면, LLM은 `author` 필드에 대한 필터(author = "김지훈")가 필요함을 추론할 수 있어야 합니다. 위의 `AttributeInfo` 리스트는 LLM에게 어떤 필드들이 있는지 알려주고 각 필드의 의미를 설명하므로, LLM이 질의에서 해당 필드와 관련된 조건을 추출하는 데 도움을 줍니다.
* **SelfQueryRetriever 생성**: `SelfQueryRetriever.from_llm(...)` 메서드를 호출하여 SelfQueryRetriever 객체를 만듭니다. 인자로:

  * `llm=chat_model`: 방금 생성한 ChatOpenAI LLM을 사용합니다. LLM은 주어진 질의를 이해하고, 아래 정보를 바탕으로 \*\*검색용 질의(query)\*\*와 \*\*필터(filter)\*\*를 생성합니다.
  * `vectorstore=vector_store`: 이전에 설정한 Pinecone 벡터 스토어를 연동합니다. LLM이 생성한 질의와 필터에 따라 이 vector\_store에서 실제 문서 검색이 일어나게 됩니다.
  * `document_contents=""`: 문서 내용 필드명을 지정하는 인자입니다. (예: 만약 문서 본문 텍스트의 필드명이 'content'라면 그걸 넣어야 하지만 여기서는 빈 문자열로 두었습니다. 기본값으로 아마 `'text'`나 해당 vector\_store의 기본 필드를 사용할 수 있습니다. 이 부분은 LangChain 설정에 따라 문서 전체 내용을 어떤 키로 참조할지 명시하는 용도인데, 이 예제에서는 문서의 주요 내용이 이미 메타데이터 'text'에 요약되어 있어서 공란으로 둔 것으로 보입니다.)
  * `metadata_field_info=metadata_field_info`: 위에서 정의한 메타데이터 필드 리스트를 전달합니다. LLM은 이 정보를 활용해 사용자 질의에서 메타데이터 관련 조건을 이해하고 쿼리에 반영합니다.
  * `verbose=True`: 동작 중 상세한 로그를 출력하도록 설정합니다. SelfQueryRetriever가 LLM을 통해 쿼리를 생성할 때 내부적으로 어떤 생각을 했는지 등 debug 정보가 출력될 수 있습니다. (예: LLM이 생성한 필터 내용이나 파싱 결과 등을 콘솔에 보여줄 수 있습니다.)

  `SelfQueryRetriever.from_llm` 함수는 내부적으로 LLM에게 "사용자 질문을 읽고, 위에서 정의한 필드들에 대해 필요한 필터 조건과 검색 키워드를 추출하라"는 프롬프트를 보내고, 그 결과를 **파싱하여** 검색 가능하게 만들어 줍니다. LangChain은 이를 위해 LLM이 특정 포맷(예: JSON이나 특정 문법)으로 답하도록 유도하고, `lark` 파서 등을 사용해 LLM 응답을 `filter` 객체와 `query` 문자열로 변환합니다. 최종적으로 이러한 설정이 모두 포함된 `self_query_retriever` 객체가 반환됩니다.
* 설정 완료 메시지 `print("Self-Query Retriever 구성 완료")`를 출력합니다.

```python
from langchain_openai import ChatOpenAI
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.schema import AttributeInfo

# ChatOpenAI 인스턴스 생성 (LLM: OpenAI Chat 모델 사용)
chat_model = ChatOpenAI(
    model_name=OPENAI_LLM_MODEL,
    openai_api_key=OPENAI_API_KEY,
    temperature=0.0
)

# 메타데이터 필드 정보 설정
metadata_field_info = [
    AttributeInfo(name='author', type='string', description='문서를 작성한 저자 이름'),
    AttributeInfo(name='category', type='list[string]', description='문서의 분류 리스트'),
    AttributeInfo(name='text', type='string', description='문서 본문 요약 또는 대표 텍스트')
]

# SelfQueryRetriever 생성 (LLM을 통해 질의 분석 및 벡터스토어 검색)
self_query_retriever = SelfQueryRetriever.from_llm(
    llm=chat_model,
    vectorstore=vector_store,
    document_contents="",          # 문서 내용 필드 (여기서는 생략 또는 기본값 사용)
    metadata_field_info=metadata_field_info,
    verbose=True
)

print("Self-Query Retriever 구성 완료")
```

## 출력 결과 5 해석: Self-Query Retriever 설정 완료 메시지

```
Self-Query Retriever 구성 완료
```

이 메시지가 나타나면 SelfQueryRetriever의 구성이 성공적으로 이루어진 것입니다. 이제 이 `self_query_retriever`는 일반적인 `retriever`처럼 동작할 수 있지만, 내부적으로는 LLM을 사용하여 질의를 처리합니다. 동작 방식은 다음과 같습니다:

* 사용자가 `self_query_retriever`에 질의를 던지면 (`self_query_retriever.retrieve(query)` 또는 이 예제에서처럼 `self_query_retriever.invoke(query)`), LLM에게 메타데이터 필드 정보와 함께 질의를 분석하도록 합니다.
* 예를 들어, 질의가 "저자 김지훈의 문서를 모두 보여줘"인 경우, LLM은 이를 읽고 `author` 필드에 `"김지훈"`이라는 값으로 필터를 만들어야겠다고 추론합니다. 또한 검색용 키워드(문서 내용 관련)가 필요하면 그것도 생성합니다 (이 경우는 모든 김지훈 문서를 달라는 것이므로 키워드보다는 필터만 중요할 것입니다).
* LLM의 응답을 LangChain이 파싱하여, 결과적으로 **벡터 검색용 쿼리**와 **Pinecone 필터 조건**이 만들어집니다. (필터란 Pinecone에 메타데이터 기반 검색을 요청할 때 사용하는 조건입니다. 예: `{"author": "김지훈"}` 같은 형태가 될 것입니다.)
* 그런 다음 `vector_store`를 사용해, 생성된 쿼리와 필터로 Pinecone에서 관련 문서를 검색합니다.
* 최종적으로 **LLM을 거쳐 향상된 검색 결과**를 얻게 됩니다.

요약하면, SelfQueryRetriever는 \*\*"사용자 질의 -> (LLM) -> 필터+벡터검색"\*\*의 단계를 거쳐서, 사용자가 묻지 않았지만 의도한 조건(이 예제에서는 저자명)을 반영한 검색을 수행하게 해줍니다.

---

## 코드 셀 6 설명: 평가 지표 계산 함수 정의

이 코드 셀에서는 검색 결과의 성능을 평가하기 위한 **함수들**을 정의합니다. 검색 모델의 성능을 측정하기 위해 정보 검색 분야의 표준 지표인 **Precision\@k, Recall\@k, MRR, MAP** 등을 계산합니다. 각각의 함수는 아래와 같습니다:

1. **`parse_relevant(relevant_str)`**: 데이터에 저장된 정답 문서 문자열(`relevant_doc_ids` 컬럼의 값)을 파싱하는 함수입니다.

   * `relevant_str`는 예를 들어 `"D1=1;D11=1;D21=1"`처럼 세미콜론으로 구분된 `"문서ID=점수"`들의 목록입니다.
   * 이 함수를 호출하면:

     * 문자열을 `';'`로 분리하여 `pairs` 리스트를 얻습니다. 예: `["D1=1", "D11=1", "D21=1"]`.
     * 각 `pair`에 대해 `'='`으로 나누어 문서 ID(`doc_id`)와 점수(`grade`)를 추출합니다.

       * `doc_id`는 `"D1"`처럼 문서의 식별자, `grade`는 `"1"`처럼 해당 문서의 관련도 점수입니다.
       * `int(grade)`로 점수를 정수로 변환합니다. (여기서는 1만 쓰이므로 결국 모든 관련 문서를 {doc\_id: 1} 형태로 담게 됩니다.)
     * 이 문서 ID들을 키로 하고 점수를 값으로 하는 딕셔너리 `rel_dict`를 만듭니다. 예: `{"D1": 1, "D11": 1, "D21": 1}`.
   * 최종적으로 `rel_dict`를 반환합니다. 이 딕셔너리는 해당 질의에 어떤 문서들이 관련 문서인지(키 존재 여부로 판단 가능) 알려주며, 값은 중요도를 나타내지만 여기서는 모두 1이라 **관련 문서 집합**으로 생각할 수 있습니다.

2. **`compute_metrics(predicted, relevant_dict, k=5)`**: 단일 질의에 대한 평가 지표(P\@k, R\@k, RR, AP)를 계산하는 함수입니다.

   * `predicted`: 검색 시스템이 반환한 **문서 ID 리스트**입니다. **순서가 중요**하며, 첫 번째 요소가 랭크 1에 해당합니다. (예: `["D11", "D21", "D1", ...]`)
   * `relevant_dict`: 앞서 `parse_relevant`로 얻은 **정답 문서 딕셔너리**입니다. 관련 문서 ID들을 키로 가지고 있습니다. (값은 중요도인데 여기선 1이라 무시해도 됨)
   * `k`: 몇 위까지 결과를 고려할지 정하는 인자입니다. 기본값 5로 설정되어 있어 Precision\@5, Recall\@5 등을 계산하게 됩니다.
   * 이 함수의 내부 작동:

     * **Hits 계산**: 상위 `k`개의 `predicted` 결과 중 정답 문서에 속하는 것이 몇 개인지 센다. (`hits`)

       * `hits = sum(1 for doc in predicted[:k] if doc in relevant_dict)`
       * 예를 들어 정답이 {D1, D11, D21}이고 예측 상위5개 중 D11, D1이 있다면 hits = 2.
     * **Precision\@k (정밀도@k)**: 상위 k개 결과 중 **관련 문서의 비율**입니다. `precision = hits / k`로 계산합니다.

       * 예: k=5, hits=2라면 P\@5 = 2/5 = 0.4 (40% 정밀도: 상위5개 중 40%가 정답).
     * **Total relevant**: 해당 질의의 전체 관련 문서 수입니다. `total_relevant = len(relevant_dict)`

       * 예: 정답 문서 3개였다면 total\_relevant = 3.
     * **Recall\@k (재현율@k)**: 전체 정답 문서 중 상위 k 안에 포함된 비율입니다. `recall = hits / total_relevant` (단, 분모가 0인 경우 0 처리).

       * 예: 정답 3개 중 2개를 상위5가 포함했다면 R\@5 = 2/3 ≈ 0.667 (66.7% 재현율: 찾을 수 있는 정답 중 66.7%를 찾음).
     * **RR (Reciprocal Rank, 역순위)**: 첫 번째로 등장한 관련 문서의 순위에 대한 역수입니다.

       * 순위는 1부터 시작하며, 함수에서는 `enumerate(predicted)`로 0부터 인덱스를 받고 있으므로 `idx+1`이 순위가 됩니다.
       * 첫 번째 관련 문서를 찾으면 `rr = 1 / rank`로 계산하고 루프를 종료합니다. 만약 관련 문서가 하나도 없으면 `rr = 0`으로 유지됩니다.
       * 예: 관련 문서들 중 가장 높은 순위가 2위에 있었다면 RR = 1/2 = 0.5, 1위에 있었다면 RR = 1.0, 관련 문서가 1위와 2위 둘 다여도 첫 관련 문서 기준으로 RR=1.0입니다.
     * **AP (Average Precision, 평균 정밀도)**: Precision\@Rank들을 평균낸 값입니다.

       * 구하는 방법: 상위 k까지 순차적으로 살펴보며, **관련 문서를 발견할 때마다** 그 지점까지의 Precision 값을 기록합니다.
       * 코드에서는 `precisions` 리스트를 이용하여, `predicted[:k]`를 순회하면서 `doc`이 관련 문서일 때마다:

         * `num_correct`를 1 증가시키고, 그 순간의 Precision (즉, `num_correct/(현재 순위)`)을 `precisions`에 추가합니다.
         * 예: 예측 순위 1\~5에서 3위와 5위 문서가 정답이었다면,

           * 3위에서 첫 정답 발견: 그때 num\_correct=1이고 rank=3이므로 precision=1/3≈0.333을 리스트 추가.
           * 4위는 정답 아님 -> 아무 추가 안함.
           * 5위에서 두번째 정답 발견: 그때 num\_correct=2, rank=5이므로 precision=2/5=0.4 추가.
         * `precisions = [0.333..., 0.4]`가 되고, AP = 이 평균 = (0.333+0.4)/2 ≈ 0.3667.
         * 만약 정답이 하나도 없으면 `precisions`가 비어 있고, AP를 0으로 처리합니다.
       * AP는 **질의별 평균 정밀도**를 의미합니다. (여기서는 상위 k=5까지만 고려한 평균 정밀도입니다.)
     * 마지막에 `(precision, recall, rr, ap)` 튜플을 반환합니다.
   * 이 함수는 하나의 질의에 대해 예측 결과와 정답을 비교해 네 가지 지표를 산출합니다.
   * 요약하면:

     * **Precision\@5**: 상위 5개 검색결과 중 정답 비율.
     * **Recall\@5**: 정답 문서들 중 상위 5개 안에 들어온 비율.
     * **MRR (Mean Reciprocal Rank)**: 가장 높은 순위로 맞힌 정답의 순위에 대한 평균적인 지표 (이 함수는 개별 질의의 Reciprocal Rank, 전체 Mean은 나중에 평균낼 예정).
     * **AP (Average Precision)**: 순위에 따라 누적되는 정밀도의 평균 (검색결과의 순위와 정답 분포를 모두 고려한 지표).

3. **`evaluate_all(results_dict, queries_df, k=5)`**: 여러 질의에 대한 평균 지표를 계산하는 함수입니다.

   * `results_dict`: 검색 시스템이 반환한 **모든 질의의 결과 묶음**입니다. 이 예제에서는 이후에 `dense_results`와 `selfquery_results`라는 딕셔너리를 만들 예정인데, 각각 `{query_id: [예측 문서ID 목록]}` 형태입니다.
   * `queries_df`: 질의 정보가 담긴 pandas 데이터프레임입니다. 이를 통해 각 질의의 정답 문서 정보를 가져옵니다.
   * `k`: 상위 몇 개 결과를 평가할지 (기본 5).
   * 이 함수는 데이터프레임을 한 행(row)씩 반복(`iterrows()`)하면서:

     * 각 질의에 대해 `query_id`와 `relevant_doc_ids`를 가져옵니다.
     * `parse_relevant`를 호출하여 해당 질의의 정답 문서 딕셔너리 `relevant`를 얻습니다.
     * `results_dict`에서 해당 질의의 예측 결과 리스트 `predicted`를 가져옵니다.
     * 앞서 정의한 `compute_metrics(predicted, relevant, k)`를 호출해 그 질의의 P, R, RR, AP 값을 계산합니다.
     * 각 값을 리스트 (`prec_list`, `rec_list`, `rr_list`, `ap_list`)에 누적하여 담습니다.
   * 모든 질의를 처리한 후, 리스트들의 평균을 내서 최종적으로 딕셔너리 형태로 반환합니다:

     * `{'P@5': 평균 Precision@5, 'R@5': 평균 Recall@5, 'MRR': 평균 RR, 'MAP': 평균 AP}`
   * 이 반환값은 주어진 검색 결과 집합에 대한 **전체 성능 지표**(평균)입니다. Precision, Recall은 @5 기준이며, MRR은 Mean Reciprocal Rank, MAP은 Mean Average Precision을 나타냅니다.
   * (용어 정리: **MRR**은 여러 질의에 대한 Reciprocal Rank의 평균으로, **MAP**은 여러 질의의 Average Precision의 평균입니다.)

```python
import numpy as np

def parse_relevant(relevant_str):
    pairs = relevant_str.split(';')
    rel_dict = {}
    for pair in pairs:
        doc_id, grade = pair.split('=')
        rel_dict[doc_id] = int(grade)
    return rel_dict

def compute_metrics(predicted, relevant_dict, k=5):
    # Precision@k: 상위 k개 예측 중 정답 수 (hits) / k
    hits = sum(1 for doc in predicted[:k] if doc in relevant_dict)
    precision = hits / k
    # Recall@k: 정답 중 상위 k에 들어온 비율
    total_relevant = len(relevant_dict)
    recall = hits / total_relevant if total_relevant > 0 else 0
    # Reciprocal Rank: 첫 번째로 맞힌 정답의 역순위
    rr = 0
    for idx, doc in enumerate(predicted):
        if doc in relevant_dict:
            rr = 1 / (idx + 1)  # (idx는 0부터 시작하므로 +1이 실제 순위)
            break
    # Average Precision: 정답을 찾을 때마다의 precision의 평균
    num_correct = 0
    precisions = []
    for i, doc in enumerate(predicted[:k]):
        if doc in relevant_dict:
            num_correct += 1
            precisions.append(num_correct / (i + 1))
    ap = np.mean(precisions) if precisions else 0
    return precision, recall, rr, ap

def evaluate_all(results_dict, queries_df, k=5):
    prec_list, rec_list, rr_list, ap_list = [], [], [], []
    for idx, row in queries_df.iterrows():
        qid = row['query_id']
        relevant = parse_relevant(row['relevant_doc_ids'])
        predicted = results_dict[qid]
        p, r, rr, ap = compute_metrics(predicted, relevant, k)
        prec_list.append(p)
        rec_list.append(r)
        rr_list.append(rr)
        ap_list.append(ap)
    return {
        'P@5': np.mean(prec_list),
        'R@5': np.mean(rec_list),
        'MRR': np.mean(rr_list),
        'MAP': np.mean(ap_list)
    }
```

**코드 실행 결과**: 이 셀은 함수 정의만을 포함하고 있으며, 호출이나 `print`가 없으므로 **별도의 출력은 발생하지 않습니다**. 다음 단계에서 이 함수들을 사용하여 성능 평가를 진행할 것입니다.

*(이 셀에서는 출력 결과가 없으므로 "출력 결과 해석" 부분은 생략합니다.)*

---

## 코드 셀 7 설명: 검색 실행 (Dense vs SelfQueryRetriever) 및 결과 수집

이 코드 셀에서는 앞서 준비한 검색 방법을 실제로 실행하여, 각 질의에 대해 **검색 결과 문서 목록**을 구합니다. 두 가지 검색 방법을 사용합니다:

* **Dense Retrieval** (`vector_store.similarity_search` 활용): 임베딩 유사도 기반 검색 (메타데이터 필터 없이, 문서 내용에 대한 벡터 검색).
* **Self-Query Retrieval** (`self_query_retriever.invoke` 활용): LLM이 질의를 이해하여 메타데이터 필터까지 적용한 검색.

코드 흐름을 상세히 살펴보면 다음과 같습니다:

* `dense_results = {}`와 `selfquery_results = {}`: 두 개의 빈 딕셔너리를 만듭니다. 이후 각 질의의 결과를 이 딕셔너리에 저장할 것입니다. 키는 `query_id`, 값은 **검색된 문서들의 ID 리스트**로 할당됩니다.
* `for idx, row in queries_df.iterrows():`: pandas 데이터프레임을 순회하면서 각 질의를 처리합니다. `row`는 현재 질의에 대한 정보 (query\_id, query\_text, relevant\_doc\_ids 등)를 담고 있습니다.

  * `qid = row['query_id']`: 현재 질의의 ID (예: "Q01")를 가져옵니다.
  * `query_text = row['query_text']`: 현재 질의의 내용 (예: "저자 김지훈의 문서를 모두 보여줘")를 가져옵니다.
  * **1) Dense Retrieval** – `vector_store.similarity_search(query_text, k=5)`:

    * `vector_store`는 앞서 Pinecone 인덱스를 감싼 객체이며, `similarity_search` 메서드는 내부적으로 `query_text`를 임베딩한 후 Pinecone에서 **코사인 유사도**가 가장 높은 상위 5개의 문서를 찾습니다 (`k=5`).
    * 결과는 LangChain의 `Document` 객체들의 리스트 `docs_dense`로 반환됩니다. 각 `Document`에는 원본 텍스트나 메타데이터가 포함되어 있습니다.
    * `dense_results[qid] = [doc.metadata['doc_id'] for doc in docs_dense]`: 검색된 Document 객체들에서 메타데이터 필드 `'doc_id'` 값을 추출하여 (예컨대 D11, D21 같은 ID들) 리스트로 만듭니다. 이 리스트를 `dense_results` 딕셔너리에 해당 질의 ID 키로 저장합니다.
    * 즉, Dense Retrieval 방식으로 찾은 **문서 ID 상위 5개**가 `dense_results`에 기록됩니다.
  * **2) Self-Query Retriever** – `docs_self = self_query_retriever.invoke(query_text)`:

    * SelfQueryRetriever의 `invoke` 메서드를 사용하여 LLM 기반 검색을 수행합니다. `query_text`를 넣으면, SelfQueryRetriever는 내부적으로 LLM에 프롬프트를 보내 필요한 **필터 조건과 벡터 쿼리**를 생성하고 Pinecone에서 검색을 합니다.
    * `docs_self`에는 검색된 Document 객체 리스트가 반환됩니다. (SelfQueryRetriever는 기본적으로 일정 수의 문서를 반환할 텐데, 명시적으로 개수를 지정하지 않으면 기본 `search_type="similarity"`에서 vector\_store의 기본값을 따릅니다. 아마도 4개 또는 5개 정도를 반환할 것입니다.)
    * `selfquery_results[qid] = [doc.metadata['doc_id'] for doc in docs_self[:5]]`: SelfQueryRetriever 결과 중 상위 5개 문서의 ID를 추출하여 리스트로 저장합니다. (만약 SelfQueryRetriever가 5개 이상 반환했다면, 동일하게 5개로 자르고, 5개 미만이면 있는 그대로 저장되겠죠.)
    * SelfQueryRetriever는 질의에 필터를 적용하기 때문에 Dense 결과와 **다른 문서들**을 반환할 수 있습니다. 예를 들어 "저자 김지훈" 질의의 경우, SelfQueryRetriever는 author 필터를 적용하여 **정확히 김지훈의 문서들**을 가져올 것이고, Dense는 텍스트 유사도로 찾기 때문에 빗나갈 가능성이 있습니다.
* 루프가 끝나면, `dense_results`와 `selfquery_results` 두 딕셔너리에 30개 질의 각각에 대한 상위 5개 검색 문서 ID 리스트가 채워지게 됩니다.
* 마지막으로 `print("검색 결과 수집 완료")`를 출력하여, 모든 질의에 대한 검색이 끝났음을 알립니다.

```python
dense_results = {}
selfquery_results = {}

for idx, row in queries_df.iterrows():
    qid = row['query_id']
    query_text = row['query_text']
    # 1) Dense Retrieval: 임베딩 유사도 검색
    docs_dense = vector_store.similarity_search(query_text, k=5)
    dense_results[qid] = [doc.metadata['doc_id'] for doc in docs_dense]
    # 2) Self-Query Retriever: LLM 기반 검색
    docs_self = self_query_retriever.invoke(query_text)
    selfquery_results[qid] = [doc.metadata['doc_id'] for doc in docs_self[:5]]

print("검색 결과 수집 완료")
```

## 출력 결과 7 해석: 검색 프로세스 완료 메시지

```
검색 결과 수집 완료
```

이 출력은 30개 질의 각각에 대해 **Dense 검색**과 **SelfQuery 검색**을 모두 수행하고, 결과를 잘 모았다는 의미입니다. 이 과정에서 시간이 조금 걸릴 수 있는데, 각 질의마다:

* Dense 검색: Pinecone에 임베딩 질의를 던져 5개 결과를 받아옴.
* SelfQuery 검색: LLM을 호출하여 필터를 만든 후 Pinecone 검색.

LLM 호출이 있기 때문에 SelfQuery 쪽은 OpenAI API 지연 등이 있을 수 있지만, 최종적으로 모든 질의를 다 처리하면 위와 같이 완료 메시지를 찍었습니다.

이 메시지만으로는 결과를 확인할 수 없으므로, 다음 단계에서 실제로 `dense_results`와 `selfquery_results`의 성능을 평가할 것입니다.

참고로, 이 시점에 `dense_results['Q01']`에는 예컨대 `["D11", "D21", "D1", "D5", "D8"]` 같은 문서 ID 리스트가 들어 있고, `selfquery_results['Q01']`에는 `["D1", "D11", "D21"]` 처럼 김지훈 저자의 문서 3개(정답들)가 정확히 들어 있을 가능성이 높습니다. (Dense는 저자명을 임베딩으로 해석하기 어려워 오답이 섞였을 수 있고, SelfQuery는 필터 덕분에 정답 위주로 나왔을 것으로 기대됩니다.)

---

## 코드 셀 8 설명: 검색 성능 평가 (Precision, Recall, MRR, MAP 계산)

이 코드 셀에서는 이전 단계에서 얻은 `dense_results`와 `selfquery_results`에 대해 **평균 성능 지표**를 계산하고 비교합니다. 다음과 같은 일을 합니다:

* `dense_metrics = evaluate_all(dense_results, queries_df, k=5)`: 우리가 정의한 `evaluate_all` 함수를 사용하여 Dense Retrieval 결과의 평균 지표를 계산합니다. `k=5`로 지정했으므로 Precision\@5, Recall\@5 등을 사용합니다. 반환되는 `dense_metrics`는 딕셔너리로, 예를 들어 `{'P@5': 0.20..., 'R@5': 0.29..., 'MRR': 0.32..., 'MAP': 0.31...}` 형식일 것입니다.
* `selfquery_metrics = evaluate_all(selfquery_results, queries_df, k=5)`: SelfQueryRetriever 결과에 대해서도 동일하게 평가합니다. 이 결과는 아마 Dense보다 훨씬 높게 나올 것으로 예상됩니다.
* 그런 다음 두 결과를 하나의 표로 보기 위해 `pandas.DataFrame`을 활용합니다:

  * `df_metrics = pd.DataFrame({...})`으로 성능 비교 테이블을 만듭니다. 이 데이터프레임은 세 개의 컬럼을 가집니다:

    * 'Metric': 지표 이름 목록 (`['P@5', 'R@5', 'MRR', 'MAP']`).
    * 'Dense': Dense 결과의 각 지표 값 리스트.
    * 'SelfQuery': SelfQuery 결과의 각 지표 값 리스트.
  * 이렇게 하면 `df_metrics` 표에는 각 행이 하나의 지표를 나타내고, Dense와 SelfQuery 열에 해당 지표의 값이 채워집니다.
* 마지막 줄 `df_metrics`를 적었으므로, 주피터 노트북은 그 데이터프레임을 예쁘게 출력해 줍니다. 이 표를 통해 두 검색 방식의 성능을 한눈에 비교할 수 있습니다.

```python
import pandas as pd

dense_metrics = evaluate_all(dense_results, queries_df, k=5)
selfquery_metrics = evaluate_all(selfquery_results, queries_df, k=5)

df_metrics = pd.DataFrame({
    'Metric': ['P@5', 'R@5', 'MRR', 'MAP'],
    'Dense': [dense_metrics['P@5'], dense_metrics['R@5'], dense_metrics['MRR'], dense_metrics['MAP']],
    'SelfQuery': [selfquery_metrics['P@5'], selfquery_metrics['R@5'], selfquery_metrics['MRR'], selfquery_metrics['MAP']]
})
df_metrics
```

## 출력 결과 8 해석: Dense vs SelfQueryRetriever 성능 비교 표

데이터프레임으로 출력된 결과는 다음과 같습니다:

| Metric | Dense             | SelfQuery         |
| ------ | ----------------- | ----------------- |
| P\@5   | 0.2067  (약 20.7%) | 0.5533  (약 55.3%) |
| R\@5   | 0.2933  (약 29.3%) | 0.9244  (약 92.4%) |
| MRR    | 0.3244            | 0.9667            |
| MAP    | 0.3133            | 0.9667            |

**표 해설:**

* **P\@5 (Precision\@5)**: Dense = **0.2067**, SelfQuery = **0.5533**
  → SelfQueryRetriever 방식이 약 **55.3%**, Dense 방식은 약 \*\*20.7%\*\*의 정밀도를 보였습니다.
  이 값은 "상위 5개의 결과 중 정답 문서의 비율"입니다. SelfQuery의 P\@5가 훨씬 높다는 것은, SelfQuery 방식으로 검색하면 상위 5개 결과 중 평균 5개 중 \*\*2.8개(55%)\*\*가 정답인 반면, Dense 방식은 **1.0개(20%)** 정도만 정답이라는 의미입니다. SelfQuery가 사용자의 의도를 더 잘 반영해 관련 문서를 상위에 많이 올려놓았음을 보여줍니다.

* **R\@5 (Recall\@5)**: Dense = **0.2933**, SelfQuery = **0.9244**
  → SelfQueryRetriever 방식이 약 **92.4%**, Dense는 약 \*\*29.3%\*\*의 재현율을 달성했습니다 (상위5 기준).
  재현율은 "전체 정답 문서들 중 몇 퍼센트를 검색 결과(상위5)에 포함시켰나"를 뜻합니다. SelfQuery의 R\@5가 0.9244라는 것은, 각 질의별로 존재하는 모든 관련 문서들 중 **92% 정도가 상위 5위 안에 검색되었다**는 뜻입니다. 거의 모든 정답을 찾아냈다고 볼 수 있습니다. 반면 Dense는 29%로 매우 낮아서, **대부분의 정답을 놓쳤다**는 해석이 됩니다. (예: 질의마다 보통 3개의 정답이 있었는데, Dense는 평균 0.88개만 찾은 셈이고, SelfQuery는 평균 2.77개를 찾은 셈입니다.)

* **MRR (Mean Reciprocal Rank)**: Dense = **0.3244**, SelfQuery = **0.9667**
  → SelfQuery가 **0.9667**, Dense는 **0.3244**로 큰 차이가 있습니다.
  Reciprocal Rank는 첫 번째로 발견된 정답의 순위에 대한 점수이며, MRR은 모든 질의에 대해 그 값을 평균낸 것입니다.
  SelfQuery의 MRR ≈ 0.967은 거의 **1.0에 가까운 값**으로, 이는 **대부분의 질의에서 첫 번째 정답 문서가 1위에 위치**함을 시사합니다. 구체적으로 MRR=0.967 정도면, 30개 질의 중 거의 모든 케이스에서 정답 문서가 랭크 1이었다고 볼 수 있습니다 (아주 일부만 랭크 2였을 가능성).
  반면 Dense의 MRR=0.324은 평균적인 첫 정답 문서 순위가 **약 1/0.324 ≈ 3**위 정도임을 의미합니다. 즉, Dense 검색은 **가장 높은 순위의 정답이 평균 3위쯤에 등장**했고, 많은 경우 1위는 엉뚱한 문서였다는 뜻입니다. SelfQuery는 사용자의 의도를 잘 파악해서 거의 항상 정답을 맨 앞에 가져온 반면, Dense는 그러지 못했습니다.

* **MAP (Mean Average Precision)**: Dense = **0.3133**, SelfQuery = **0.9667**
  → SelfQueryRetriever의 MAP이 **0.9667**로 매우 높고, Dense의 MAP은 **0.3133**에 그쳤습니다.
  MAP는 모든 검색 결과 순위에서 정답들의 분포를 고려한 종합적인 정밀도 지표입니다. SelfQuery의 MAP ≈ 0.967이라는 것은, **검색 결과 전반에 걸쳐 정답 문서들을 거의 완벽하게 상위에 배치**했다는 뜻입니다. Dense의 0.313은 정답을 상위에 잘 못 올려놓았다는 의미죠.
  SelfQuery의 MAP과 MRR이 똑같이 높은 것은, 이 검색 문제에서 각 질의마다 정답 문서가 3개 정도로 비교적 적고 SelfQuery가 그 3개를 모두 5위 안에, 특히 1\~2위에 잘 넣어주었기 때문으로 해석됩니다. Dense는 정답 3개 중 1개 정도만 상위권에, 나머지는 순위 밖이거나 후순위에 있어서 평균 정밀도가 낮게 나온 것입니다.

**결론:** SelfQueryRetriever를 사용한 검색은 **정확도와 재현율 면에서 Dense 임베딩 검색보다 월등히 뛰어나다**는 것이 수치로 확인되었습니다. 특히 이 데이터셋은 "저자 OOO의 문서"처럼 **메타데이터(저자) 기반 조건**이 중요한 질의들이었기 때문에, LLM이 그런 조건을 인식해 필터로 활용한 SelfQuery 방식이 큰 효과를 봤습니다. SelfQueryRetriever는 저자 이름을 필터로 사용하여 **해당 저자의 문서만을 집중적으로 찾아내므로**, 거의 모든 정답을 상위에 올릴 수 있었습니다. 반면 Dense Retrieval는 질의문 전체를 임베딩으로 처리하기 때문에, "김지훈"이라는 사람이름이 문서 내용에 충분히 드러나지 않았다면 정확히 그 저자의 문서를 찾기 어려웠던 것으로 보입니다. 이로 인해 정답을 많이 놓친 것으로 분석됩니다.

---

## 코드 셀 9 설명: 성능 비교 결과 시각화 (그래프 그리기)

마지막 코드 셀에서는 앞서 표로 정리한 성능 지표를 \*\*라인 차트(line chart)\*\*로 시각화합니다. 그래프를 통해 Dense 검색과 SelfQuery 검색의 성능 차이를 한눈에 비교할 수 있습니다. 주요 내용은:

* 한글 폰트 설정: Windows 환경에 있는 `'malgun.ttf'` (맑은 고딕 폰트)를 지정하여 `matplotlib` 그래프에서 한글이 깨지지 않도록 합니다. (`FontProperties`와 `rc`를 사용) 그리고 `plt.rcParams['axes.unicode_minus'] = False`로 설정하여 마이너스 기호 깨짐을 방지합니다.
* `methods = ['Dense', 'SelfQuery']`: 범례(legend)에 사용될 두 방법의 이름 리스트입니다.
* `metrics_list = ['P@5', 'R@5', 'MRR', 'MAP']`: x축에 표시할 **성능 지표들**의 이름입니다. 앞서 표의 Metric 열과 동일합니다.
* `dense_vals = [dense_metrics['P@5'], ...]`, `self_vals = [selfquery_metrics['P@5'], ...]`: 두 리스트에는 각 지표에 대한 Dense와 SelfQuery의 수치를 담았습니다. 예를 들어 `dense_vals = [0.2067, 0.2933, 0.3244, 0.3133]`, `self_vals = [0.5533, 0.9244, 0.9667, 0.9667]` 정도가 될 것입니다.
* `x = range(len(metrics_list))`: x축 상의 위치를 0,1,2,3 으로 생성합니다 (지표 4개).
* `plt.figure(figsize=(8,5))`: 그림 사이즈를 가로 8인치, 세로 5인치로 설정합니다.
* `plt.plot(x, dense_vals, marker='o', label='Dense')`: Dense 결과 값을 y축으로 하여 선 그래프를 그립니다. `marker='o'`로 각 점에 동그라미 표시를 하고, label을 'Dense'로 지정합니다.
* `plt.plot(x, self_vals, marker='s', label='SelfQuery')`: SelfQuery 결과에 대해서도 같은 방식으로 그립니다. `marker='s'`는 사각형 포인트입니다.
* `plt.xticks(x, metrics_list)`: x축 눈금을 0,1,2,3에서 해당 지표 이름 'P\@5','R\@5','MRR','MAP'으로 바꿔서 표시합니다.
* `plt.ylim(0,1)`: y축 범위를 0부터 1까지로 제한하여, 모든 지표 값(0\~1 사이)이 그래프 전체 높이를 활용하도록 합니다.
* `plt.xlabel('지표')`, `plt.ylabel('점수')`: x축, y축 라벨을 한글로 적습니다. ('지표' = Metric, '점수' = Score)
* `plt.title('Dense vs Self-Query Retriever 성능 비교')`: 그래프의 제목을 한글로 적습니다.
* `plt.legend()`: 앞서 지정한 label에 따라 범례(노란색 = Dense, 주황색 = SelfQuery)를 표시합니다.
* `plt.grid(True)`: 격자선을 그려 그래프를 읽기 쉽게 합니다.
* `plt.show()`: 시각화를 출력합니다. Jupyter 환경에서는 이때 그래프 이미지가 노트북에 나타나게 됩니다.

```python
import matplotlib.pyplot as plt
import matplotlib.font_manager as font_manager
from matplotlib import rc

# 한글 폰트 설정 (Windows 환경 예시, 맑은 고딕)
font_path = "C:/Windows/Fonts/malgun.ttf"
font_prop = font_manager.FontProperties(fname=font_path).get_name()
rc('font', family=font_prop)
plt.rcParams['axes.unicode_minus'] = False

methods = ['Dense', 'SelfQuery']
metrics_list = ['P@5', 'R@5', 'MRR', 'MAP']
dense_vals = [dense_metrics['P@5'], dense_metrics['R@5'], dense_metrics['MRR'], dense_metrics['MAP']]
self_vals = [selfquery_metrics['P@5'], selfquery_metrics['R@5'], selfquery_metrics['MRR'], selfquery_metrics['MAP']]

x = range(len(metrics_list))
plt.figure(figsize=(8,5))
plt.plot(x, dense_vals, marker='o', label='Dense')
plt.plot(x, self_vals, marker='s', label='SelfQuery')
plt.xticks(x, metrics_list)
plt.ylim(0,1)
plt.xlabel('지표')
plt.ylabel('점수')
plt.title('Dense vs Self-Query Retriever 성능 비교')
plt.legend()
plt.grid(True)
plt.show()
```

## 출력 결과 9 해석: Dense vs SelfQueryRetriever 성능 그래프

위 그래프는 앞서 표로 본 성능 지표들을 시각적으로 비교한 것입니다. 가로축은 \*\*성능 지표 종류(P\@5, R\@5, MRR, MAP)\*\*이고, 세로축은 \*\*점수(0\~1)\*\*입니다. 노란색 ● 선은 Dense Retrieval의 성능, 주황색 ■ 선은 SelfQueryRetriever의 성능을 나타냅니다.

그래프를 해석해보면:

* \*\*Precision\@5 (P\@5)\*\*에서: 노란 선(Dense)은 약 0.21 위치에, 주황 선(SelfQuery)은 약 0.55 위치에 점이 찍혀 있습니다. SelfQuery가 Dense보다 훨씬 높은 정밀도를 보입니다.
* **Recall\@5 (R\@5)**: 노란 선은 약 0.29, 주황 선은 약 0.92로, 두 값 사이의 거리가 매우 큽니다. SelfQuery 방법이 현저히 높은 재현율을 보임을 한눈에 알 수 있습니다.
* **MRR**: Dense 약 0.32, SelfQuery 약 0.97로, 역시 SelfQuery 곡선이 거의 맨 위에 가 있습니다.
* **MAP**: Dense 약 0.31, SelfQuery 약 0.97로, MRR과 비슷한 패턴입니다.

전체적으로 **주황색 SelfQuery 곡선이 노란색 Dense 곡선보다 모든 지표에서 훨씬 위에 위치**하고 있습니다. 특히 MRR과 MAP의 경우 SelfQuery 곡선이 거의 1.0에 도달하여 상단에 평평하게 그려져 있는데, 이는 SelfQueryRetriever의 성능이 대부분의 지표에서 **매우 우수**함을 의미합니다. Dense의 곡선은 0.2\~0.4 부근에 머무르고 있어 상당히 낮은 성능을 시각적으로 보여줍니다.

이 시각화는 표 숫자보다도 직관적으로 두 방법의 격차를 드러냅니다. SelfQueryRetriever를 사용하면 검색 시스템의 정밀도와 재현율이 크게 향상되고, 사용자의 조건(예에서는 "저자")을 정확히 반영하여 **관련 있는 문서를 최상위에 노출**시킬 수 있다는 점을 확인할 수 있습니다. Dense 임베딩 검색만으로는 메타데이터 정보를 활용하지 못해 성능이 떨어지지만, LLM의 힘을 빌려 **질의 의미를 파악하고 구조화**하면 훨씬 나은 결과를 얻을 수 있다는 결론을 얻을 수 있습니다.
