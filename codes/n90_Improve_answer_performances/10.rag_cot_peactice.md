# RAG CoT 프롬프트 엔지니어링 실습


소단계별(Easy/Medium/Hard) RAG 파이프라인에서 CoT 프롬프트와 LCEL 체이닝을 익히는 실습입니다.

## 1. 공통 설정

이 첫 번째 코드 셀은 실습 전체에 공통으로 필요한 **환경 설정과 도구 준비**를 합니다.
여기서는 대형 언어 모델(LLM)을 초기화하고, 질의에 관련된 문서를 찾아주는 **retriever**와 LLM의 출력을 처리할 \*\*출력 파서(Output Parser)\*\*를 정의합니다. 이 설정을 통해 이후 단계에서 모델이 외부 지식을 활용하고 단계별 답안을 생성할 수 있는 기반을 마련합니다.

* **환경 변수 로드**: `load_dotenv()` 함수를 사용하여 `.env` 파일에 저장된 설정값을 불러옵니다. 이를 통해 OpenAI API 키와 사용할 LLM 모델 이름 등을 프로그램에 환경 변수로 설정합니다. (예: `OPENAI_API_KEY`와 `OPENAI_LLM_MODEL`을 가져옴)
* **LLM 초기화**: `ChatOpenAI` 클래스를 이용해 OpenAI의 챗 모델을 초기화합니다. 여기서는 환경 변수에서 모델 이름과 API 키를 읽어와 설정하며, `temperature=0.0`으로 지정하여 **출력의 무작위성 없이 일관된 답변**을 얻도록 했습니다. (Temperature를 0으로 하면 매번 같은 질문에 같은 답을 하도록 제어합니다)
* **가짜 Retriever 정의**: **RAG** 기법(Retrieval-Augmented Generation, *검색 증강 생성*)에서 **retriever**는 질문과 관련된 문서를 검색해 주는 역할을 합니다. 이 예제에서는 간단히 미리 준비된 `docs` 딕셔너리에 세 개의 문서를 담아 두고, `fake_retriever` 함수를 통해 항상 상위 3개 문서를 반환하도록 했습니다. 실제로는 검색 엔진이나 벡터 DB를 활용해 **관련 문서**를 찾겠지만, 여기서는 **고정된 작은 지식 베이스**를 사용해 검색을 흉내 냅니다. (마치 모델이 답하기 전에 작은 백과사전에서 필요한 정보를 꺼내오는 것과 비슷한 접근입니다)
* **최종 답변 파서 정의**: `FinalAnswerParser` 클래스는 LangChain의 `BaseOutputParser`를 상속하여 만들어졌으며, LLM의 긴 답변 중에서 **최종 답변 부분만 추출**하는 도구입니다. `parse` 메서드에서는 정규표현식(regex)을 사용해 답변 텍스트에서 `3. **최종 답변**:`으로 시작하는 부분을 찾아내서, 그 이후의 텍스트만 반환합니다. 만약 해당 패턴이 없으면 조금 더 포괄적인 패턴(`"최종 답변:"`)을 찾아보고, 그래도 없으면 전체 답변을 그대로 반환합니다. 이 출력 파서를 활용하면 여러 단계로 구성된 답변 중에서도 최종 결론만 손쉽게 얻을 수 있습니다.

이렇게 환경 설정과 함수를 정의해두면, 이후 Easy/Medium/Hard 단계의 질의 응답에서 **공통으로** LLM과 retriever, 파서 기능을 사용하여 답변을 생성할 수 있게 됩니다.

```python
# .env 로드 & LLM 초기화
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

load_dotenv()
llm = ChatOpenAI(
    model_name=os.getenv("OPENAI_LLM_MODEL"),
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    temperature=0.0
)

# 가짜 Retriever (Top-3 문서 반환)
docs = {
    "doc1": "대한민국의 수도는 서울입니다. 서울은 한강을 끼고 발달한 도시입니다.",
    "doc2": "서울의 대표적 관광지는 경복궁, 남산타워, 명동 등이 있습니다.",
    "doc3": "서울의 인구는 약 천만 명이며, 교통·문화 인프라가 잘 갖춰져 있습니다."
}
def fake_retriever(query: str, top_k: int = 3):
    return [docs[f"doc{i}"] for i in range(1, top_k+1)]

# OutputParser 정의
import re
from langchain.schema import BaseOutputParser

class FinalAnswerParser(BaseOutputParser):
    def parse(self, text: str) -> str:
        pattern = r"3\.\s*\*\*?최종\s*답변\*{0,2}[:\s]*(.+)$"
        m = re.search(pattern, text, re.DOTALL | re.MULTILINE)
        if m:
            return m.group(1).strip()
        fb = re.search(r"최종\s*답변[:\s]*(.+)", text, re.DOTALL)
        return fb.group(1).strip() if fb else text.strip()
```

## 2. 난이도 하 (Easy)

* 질의 복잡도: 단일 사실 조회
* 예제: '서울의 수도는 어디인가요?'

이제 **난이도 하(Easy)** 단계 예제로, "서울의 수도는 어디인가요?"라는 비교적 단순한 질문을 풀어보겠습니다. 이 셀의 코드는 **검색 증강 생성(RAG)** 파이프라인과 **사고의 사슬(CoT) 프롬프팅** 기법을 활용하여, 주어진 질문에 단계적으로 답을 구하는 과정을 보여줍니다.

1. **질의 및 문서 검색**: `query` 변수에 질문을 문자열로 정의하고, 앞서 만든 `fake_retriever`를 사용해 관련 문서를 검색합니다. `fake_retriever(query, top_k=2)`는 미리 준비된 문서들 중에서 상위 2개의 문서를 가져오는데, 여기서는 `doc1`과 `doc2`가 사용됩니다. (필요한 경우 `doc3`은 빈 문자열로 채워집니다) 이렇게 **검색된 문서들**이 있어야 모델이 자신의 지식만으로는 모를 수 있는 최신 정보나 세부 사실도 답변에 활용할 수 있습니다.
2. **프롬프트 템플릿 생성**: `PromptTemplate`을 이용해 LLM에 전달할 프롬프트 양식을 만듭니다. 이 템플릿에는 세 개의 문서(`{doc1}`, `{doc2}`, `{doc3}`) 내용과 사용자 질문(`{question}`)이 삽입될 자리표시자가 있고, 그 아래에 `[지시사항]`으로 **모델이 따라야 할 단계별 지침**을 작성했습니다. 지시사항에는 **1단계: 핵심 정보 추출**, **2단계: 정보 조합**, **3단계: 최종 답변 작성**의 흐름이 명시되어 있습니다. 이러한 프롬프트 설계는 모델에게 생각의 순서를 제시하는 것으로, **사고의 사슬(Chain-of-Thought)** 프롬프팅의 일종입니다. 즉, 모델이 답을 바로 내지 않고 먼저 문서에서 중요한 정보를 뽑고, 이를 종합한 뒤 마지막으로 답을 제시하도록 유도하는 것입니다. (마치 사람에게 "먼저 자료를 읽고 요점을 정리한 다음 최종 답을 말해봐"라고 안내하는 것과 비슷합니다.)
3. **체인 구성 (LCEL 활용)**: `prompt_easy | llm | StrOutputParser()` 구문은 LangChain Expression Language를 사용한 **체인 구성**입니다. 앞에서 준비한 프롬프트 템플릿(`prompt_easy`)의 출력이 곧바로 LLM의 입력으로 들어가고, 이어서 LLM의 출력이 `StrOutputParser()`로 넘어가도록 **파이프라인**을 만든 것입니다. 여기서 `|` (파이프 연산자)는 각 객체의 처리를 순차적으로 연결해 주며, `StrOutputParser`는 특별한 가공 없이 LLM의 출력 문자열을 그대로 전달하는 역할을 합니다. (참고로, 주석 처리된 `FinalAnswerParser`를 대신 사용하면 모델 출력에서 최종 답변만 추출할 수도 있습니다.)
4. **체인 실행 및 결과 출력**: `chain_easy.invoke({...})`를 호출하여 체인을 실제 실행합니다. 이때 딕셔너리 형태로 문서 내용과 질문을 채워 넣으면, 체인이 알아서 프롬프트 생성 → LLM 실행 → 결과 파싱의 단계를 거칩니다. 최종적으로 `print`를 통해 결과를 출력하는데, **모델의 사고 과정과 최종 답변**이 모두 포함된 텍스트를 확인할 수 있습니다. 예를 들어, 모델은 1단계에서는 "서울은 대한민국의 수도입니다."와 같은 핵심 정보를 찾아내고, 2단계에서는 추가 정보를 조합한 문장을 만든 뒤, 3단계에서 최종 답변을 제시합니다. 이처럼 Easy 단계에서는 단일 사실을 묻는 질문에 대해 **문서에서 필요한 한 가지 정보만 추출하여 답하는 과정**을 볼 수 있습니다.

```python
query = "서울의 수도는 어디인가요?"
retrieved = fake_retriever(query, top_k=2)

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt_easy = PromptTemplate(
    input_variables=["doc1", "doc2", "doc3", "question"],
    template="""=== 문서1 ===
{doc1}

=== 문서2 ===
{doc2}

=== 문서3 ===
{doc3}

질문: {question}

[지시사항]
1. 핵심 정보 추출  
2. 정보를 조합  
3. **최종 답변**:
"""
)

chain_easy = prompt_easy | llm | StrOutputParser()
# chain_easy = prompt_easy | llm | FinalAnswerParser()
result_easy = chain_easy.invoke({
    "doc1": retrieved[0],
    "doc2": retrieved[1],
    "doc3": retrieved[2] if len(retrieved) > 2 else "",
    "question": query
})
print("Easy 결과:", result_easy)
```

## 3. 난이도 중 (Medium)

* 질의 복잡도: 비교·정리
* 예제: '서울의 주요 관광지 3곳과 특징을 알려주세요.'

**난이도 중(Medium)** 단계에서는 질문이 조금 더 복잡해집니다. 이번 셀에서는 "서울의 주요 관광지 3곳과 특징을 알려주세요."라는 질문에 답하기 위해, 여러 문서의 정보를 **비교·정리**하여 최종 답변을 구성합니다. Easy 단계보다 한 걸음 나아가, **여러 개의 사실을 종합하고 결과를 표로 정리**하는 과정을 보여줍니다.

* **질문 이해**: 이 질문은 서울의 대표적인 관광지 세 곳과 각각의 특징을 묻고 있으므로, 단순 사실 한 가지가 아니라 여러 정보를 찾아 **종합적으로 정리**해야 합니다.
* **문서 검색**: `fake_retriever(query, top_k=3)`으로 세 개의 문서를 모두 불러옵니다. (doc1, doc2, doc3 모두 사용) 각 문서는 서울의 다른 측면(수도 정보, 관광지 목록, 인구와 인프라)에 관한 내용을 담고 있습니다. 모델은 이들 문서를 참고하여 답을 준비하게 됩니다.
* **프롬프트 템플릿 작성**: `prompt_med` 템플릿에서는 Easy 단계보다 상세한 지시사항이 포함됩니다. `"문서들을 읽고 단계별로 답하세요."`라는 안내 문구로 시작하여, 이어서 문서1\~3의 내용과 질문이 제공됩니다. 그런 다음 `[지시사항]`에는:

  1. 각 문서에서 **핵심 키워드 추출**,
  2. **관광지별 특징 정리**,
  3. **최종 답변을 표 형태로 제시**
     라고 명시되어 있습니다. 이처럼 모델에게 **출력 형식까지 구체적으로 지시**함으로써, 답변을 표(Table)로 만들어 내도록 유도하고 있습니다. CoT 프롬프팅 기법이 여기서도 적용되어, 모델이 먼저 각 문서의 중요한 키워드를 뽑고, 다음으로 그 정보를 가지고 관광지별 특징을 작성한 후, 마지막에 요구 형식(표)에 맞춰 답변을 종합하도록 한 것입니다.
* **체인 실행**: Easy 단계와 동일하게 `chain_med = prompt_med | llm | StrOutputParser()`로 체인을 구성하고, `.invoke({...})`로 실제 실행하여 결과를 얻습니다. 출력 결과를 보면 모델이 지시사항에 맞춰:

  1. 먼저 각 문서에서 서울과 관련된 핵심 키워드들을 나열하고,
  2. 이어서 관광지 세 곳(경복궁, 남산타워, 명동)의 특징을 한 문장씩 정리한 뒤,
  3. **최종 답변**으로 마크다운 표 형식의 결과를 제시합니다.
     표에는 관광지 이름과 그 특징이 정돈되어 나타나는데, 이로써 사용자는 질문에 대한 명확한 비교 정리가 된 답변을 얻을 수 있습니다.

Medium 단계는 여러 자료를 **분석 및 정리**하는 예제로, 모델이 단순 사실 이상의 작업(여러 문서에서 정보 추출 및 통합)을 수행하도록 프롬프트를 설계한 점이 특징입니다. 이를 통해 프롬프트 엔지니어링을 활용하면 **구조화된 출력**까지 이끌어낼 수 있음을 보여줍니다.

```python
query = "서울의 주요 관광지 3곳과 특징을 알려주세요."
retrieved = fake_retriever(query, top_k=3)

prompt_med = PromptTemplate(
    input_variables=["doc1","doc2","doc3","question"],
    template="""문서들을 읽고 단계별로 답하세요.

=== 문서1 ===
{doc1}

=== 문서2 ===
{doc2}

=== 문서3 ===
{doc3}

질문: {question}

[지시사항]
1) 각 문서에서 핵심 키워드 추출  
2) 관광지별 특징 정리  
3) **최종 답변**을 표 형태로 제시
"""
)

chain_med = prompt_med | llm | StrOutputParser()
# chain_med = prompt_med | llm | FinalAnswerParser()
result_med = chain_med.invoke({
    "doc1": retrieved[0],
    "doc2": retrieved[1],
    "doc3": retrieved[2] if len(retrieved) > 2 else "",
    "question": query
})
print("Medium 결과:", result_med)
```

## 4. 난이도 상 (Hard)

* 질의 복잡도: 복합 추론
* 예제: '서울의 인구, 관광지, 교통 인프라를 종합해 여행하기 좋은 이유를 논리적으로 설명해주세요.'

**난이도 상(Hard)** 단계는 가장 복합적인 문제를 다룹니다. 질문은 "서울의 인구, 관광지, 교통 인프라를 종합해 여행하기 좋은 이유를 논리적으로 설명해주세요."로, 서울이라는 도시가 여행하기 좋다는 것을 **여러 측면의 데이터를 근거로 논증**해야 합니다. 이 셀의 코드는 고난도 질문에 대해 체계적인 사고 과정을 거쳐 **논리적인 에세이 형식의 답변**을 생성하는 방법을 보여줍니다.

* **문서 검색**: `fake_retriever(query, top_k=3)`를 통해 세 개의 문서를 모두 검색하여 활용합니다. 각 문서에는 서울의 **인구**, **관광지**, **교통 인프라**에 대한 정보가 각각 들어있으므로, 이 모두를 고려해야 종합적인 답변을 만들 수 있습니다.
* **프롬프트 템플릿 작성**: `prompt_hard`에는 심층 분석을 위한 지시사항이 단계별로 포함되어 있습니다.

  1. **문서별 핵심 데이터 정리**: 각 문서에서 중요한 데이터를 뽑아 정리하기 (예: 문서1에서 수도/위치, 문서2에서 주요 관광지, 문서3에서 인구와 인프라 정보 요약)
  2. **데이터 간 상호관계 분석**: 문서들에서 얻은 정보를 서로 연관지어 분석하기 (예: 인구와 관광지의 관계, 관광지와 교통 인프라의 관계 등)
  3. **분석 기반 이유 서술**: 앞서 분석한 내용을 바탕으로, "왜 서울이 여행하기 좋은지" 그 이유를 논리 정연하게 서술하기
  4. **최종 답변**: 위의 내용을 종합하여 **서론·본론·결론 형식**으로 최종 답변 제시하기
     이처럼 네 단계 지침을 통해 모델에게 **논증 구조로 사고하도록 유도**하고 있습니다. 마치 사람에게 "자료를 요약하고, 서로 관계를 생각해 보고, 그 근거로 문단을 작성한 다음, 서론/본론/결론으로 글을 완성해라"라고 과제를 내는 것과 비슷한 맥락입니다.
* **체인 실행**: 이전과 마찬가지로 체인을 구성하여 실행하고 결과를 출력합니다. 모델의 출력 결과를 살펴보면, 지시에 따라:

  * 먼저 각 문서의 핵심 내용을 bullet 포인트로 정리하고,
  * 다음으로 각 요소들(인구, 관광지, 교통 인프라)이 서로 어떻게 영향을 주는지에 대한 분석을 나열한 뒤,
  * 그런 분석을 근거로 "서울이 왜 여행하기 좋은지"를 한 단락으로 서술하고,
  * 마지막으로 **서론, 본론, 결론**으로 구성된 최종 답변을 제시합니다.
    최종 답변 부분은 하나의 짧은 글처럼 구조화되어 있으며, 앞서 추출한 내용들이 **논리적인 흐름**으로 연결되어 있습니다. 예를 들어, 서론에서는 서울의 전반적 소개(인구 많고 관광지 많으며 교통 좋다는 언급)를 하고, 본론에서는 인구→문화 활기, 관광지→접근성, 인구→교통 발전 등의 세부 논거를 설명하며, 결론에서는 이러한 점들을 종합하여 서울이 여행하기 좋다고 결론짓습니다.

가장 어려운 Hard 단계 예제를 통해, 복잡한 질문도 CoT 프롬프트 설계를 통해 여러 자료를 **체계적으로 분석**하고 **설득력 있는 서술**로 답변할 수 있음을 확인할 수 있습니다. 모델이 단순 Q\&A를 넘어, 주어진 자료를 논리적으로 엮어가며 마치 인간이 작성한 짧은 글처럼 답변을 생성해낸다는 점이 강조됩니다.

```python
query = "서울의 인구, 관광지, 교통 인프라를 종합해 여행하기 좋은 이유를 논리적으로 설명해주세요."
retrieved = fake_retriever(query, top_k=3)

prompt_hard = PromptTemplate(
    input_variables=["doc1","doc2","doc3","question"],
    template="""아래 문서들을 바탕으로 심층 분석을 수행하세요.

=== 문서1 ===
{doc1}

=== 문서2 ===
{doc2}

=== 문서3 ===
{doc3}

질문: {question}

[지시사항]
1. 문서별 핵심 데이터(인구·관광·교통) 정리  
2. 데이터 간 상호관계 분석  
3. 분석 기반 이유 서술  
4. **최종 답변**: 서론·본론·결론 구조로 제시
"""
)

chain_hard = prompt_hard | llm | StrOutputParser()
# chain_hard = prompt_hard | llm | FinalAnswerParser()
result_hard = chain_hard.invoke({
    "doc1": retrieved[0],
    "doc2": retrieved[1],
    "doc3": retrieved[2] if len(retrieved) > 2 else "",
    "question": query
})
print("Hard 결과:", result_hard)
```
