{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24bc53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install python-dotenv pandas pinecone langchain langchain-openai langchain-pinecone scikit-learn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "babebe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "환경 변수 로딩 완료 : ir-embeddings, us-east-1, aws\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_LLM_MODEL = os.getenv(\"OPENAI_LLM_MODEL\")  # 'gpt-4o-mini'\n",
    "OPENAI_EMBEDDING_MODEL = os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_REGION = os.getenv(\"PINECONE_INDEX_REGION\")\n",
    "PINECONE_INDEX_CLOUD = os.getenv(\"PINECONE_INDEX_CLOUD\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")  # 'ir'\n",
    "PINECONE_INDEX_METRIC = os.getenv(\"PINECONE_INDEX_METRIC\")\n",
    "PINECONE_INDEX_DIMENSION = int(os.getenv(\"PINECONE_INDEX_DIMENSION\"))\n",
    "\n",
    "# 압축 인덱스 이름\n",
    "COMPRESSED_INDEX_NAME = f\"{PINECONE_INDEX_NAME}-compressed\"\n",
    "\n",
    "print(f\"환경 변수 로딩 완료 : {PINECONE_INDEX_NAME}, {PINECONE_INDEX_REGION}, {PINECONE_INDEX_CLOUD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0e6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 수: 30\n",
      "질의 수: 30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "documents_df = pd.read_csv(\"../../datas/documents.csv\")\n",
    "queries_df = pd.read_csv(\"../../datas/queries.csv\")\n",
    "\n",
    "print(f\"문서 수: {len(documents_df)}\")\n",
    "print(f\"질의 수: {len(queries_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c215813c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약용 LangChain 체인 구성 완료\n",
      "GPT-4o-mini 기반 문서 압축 완료\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ChatOpenAI 인스턴스\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=OPENAI_LLM_MODEL,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# PromptTemplate 설정\n",
    "summarize_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"\n",
    "아래 문서를 읽고, 핵심 내용을 짧고 간결하게 요약하세요:\n",
    "\n",
    "{text}\n",
    "\n",
    "요약:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# StrOutputParser 설정\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 체인 구성\n",
    "summarization_chain = summarize_prompt | chat_model | output_parser\n",
    "print(\"요약용 LangChain 체인 구성 완료\")\n",
    "\n",
    "# 문서별 요약 생성\n",
    "compressed_texts = []\n",
    "for idx, row in documents_df.iterrows():\n",
    "    doc_id = row['doc_id']\n",
    "    content = row['content']\n",
    "    summary = summarization_chain.invoke({\"text\": content})\n",
    "    compressed_texts.append({'doc_id': doc_id, 'content': summary})\n",
    "    time.sleep(1)  # 호출 제한 관리\n",
    "\n",
    "compressed_df = pd.DataFrame(compressed_texts)\n",
    "print(\"GPT-4o-mini 기반 문서 압축 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c84a1943",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_df.to_csv(\"../../datas/compressed_documents.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b953d6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone 연결 및 벡터 스토어 설정 완료\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Pinecone 클라이언트 연결\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# 압축 인덱스 없으면 생성\n",
    "if COMPRESSED_INDEX_NAME not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=COMPRESSED_INDEX_NAME,\n",
    "        dimension=PINECONE_INDEX_DIMENSION,\n",
    "        metric=PINECONE_INDEX_METRIC,\n",
    "        spec=ServerlessSpec(region=PINECONE_INDEX_REGION, cloud=PINECONE_INDEX_CLOUD)\n",
    "    )\n",
    "compressed_index = pc.Index(COMPRESSED_INDEX_NAME)\n",
    "\n",
    "# 임베딩 모델 생성\n",
    "embedding_model = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL, openai_api_key=OPENAI_API_KEY)\n",
    "# 원본 벡터 스토어 (기존 ir)\n",
    "vector_store = PineconeVectorStore(index_name=PINECONE_INDEX_NAME, embedding=embedding_model)\n",
    "# 압축 벡터 스토어\n",
    "compressed_vector_store = PineconeVectorStore(index_name=COMPRESSED_INDEX_NAME, embedding=embedding_model)\n",
    "\n",
    "print(\"Pinecone 연결 및 벡터 스토어 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "524b9e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "압축 문서 업서트 완료\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# 압축 문서를 compressed index에 업서트\n",
    "docs_to_upsert = []\n",
    "for idx, row in compressed_df.iterrows():\n",
    "    docs_to_upsert.append(Document(page_content=row['content'], metadata={'doc_id': row['doc_id']}))\n",
    "compressed_vector_store.add_documents(docs_to_upsert)\n",
    "print(\"압축 문서 업서트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84566fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parse_relevant(relevant_str):\n",
    "    pairs = relevant_str.split(';')\n",
    "    rel_dict = {}\n",
    "    for pair in pairs:\n",
    "        doc_id, grade = pair.split('=')\n",
    "        rel_dict[doc_id] = int(grade)\n",
    "    return rel_dict\n",
    "\n",
    "def compute_metrics(predicted, relevant_dict, k=5):\n",
    "    hits = sum(1 for doc in predicted[:k] if doc in relevant_dict)\n",
    "    precision = hits / k\n",
    "    total_relevant = len(relevant_dict)\n",
    "    recall = hits / total_relevant if total_relevant > 0 else 0\n",
    "    rr = 0\n",
    "    for idx, doc in enumerate(predicted):\n",
    "        if doc in relevant_dict:\n",
    "            rr = 1 / (idx + 1)\n",
    "            break\n",
    "    num_correct = 0\n",
    "    precisions = []\n",
    "    for i, doc in enumerate(predicted[:k]):\n",
    "        if doc in relevant_dict:\n",
    "            num_correct += 1\n",
    "            precisions.append(num_correct / (i + 1))\n",
    "    ap = np.mean(precisions) if precisions else 0\n",
    "    return precision, recall, rr, ap\n",
    "\n",
    "def evaluate_all(results_dict, queries_df, k=5):\n",
    "    prec_list, rec_list, rr_list, ap_list = [], [], [], []\n",
    "    for idx, row in queries_df.iterrows():\n",
    "        qid = row['query_id']\n",
    "        relevant = parse_relevant(row['relevant_doc_ids'])\n",
    "        predicted = results_dict[qid]\n",
    "        p, r, rr, ap = compute_metrics(predicted, relevant, k)\n",
    "        prec_list.append(p)\n",
    "        rec_list.append(r)\n",
    "        rr_list.append(rr)\n",
    "        ap_list.append(ap)\n",
    "    return {\n",
    "        'P@5': np.mean(prec_list),\n",
    "        'R@5': np.mean(rec_list),\n",
    "        'MRR': np.mean(rr_list),\n",
    "        'MAP': np.mean(ap_list)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7781f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 결과 수집 완료\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Original</th>\n",
       "      <th>Compressed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P@5</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R@5</td>\n",
       "      <td>1.716667</td>\n",
       "      <td>0.816667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MRR</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MAP</td>\n",
       "      <td>0.978148</td>\n",
       "      <td>0.818519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Metric  Original  Compressed\n",
       "0    P@5  0.466667    0.240000\n",
       "1    R@5  1.716667    0.816667\n",
       "2    MRR  0.977778    0.833333\n",
       "3    MAP  0.978148    0.818519"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원본(index 'ir')과 압축(index 'ir-compressed')에서 검색 수행\n",
    "orig_results = {}\n",
    "comp_results = {}\n",
    "for idx, row in queries_df.iterrows():\n",
    "    qid = row['query_id']\n",
    "    query_text = row['query_text']\n",
    "    # 원본 검색\n",
    "    docs_orig = vector_store.similarity_search(query_text, k=5)\n",
    "    orig_results[qid] = [doc.metadata['doc_id'] for doc in docs_orig]\n",
    "    # 압축 검색\n",
    "    docs_comp = compressed_vector_store.similarity_search(query_text, k=5)\n",
    "    comp_results[qid] = [doc.metadata['doc_id'] for doc in docs_comp]\n",
    "\n",
    "print(\"검색 결과 수집 완료\")\n",
    "# 평가\n",
    "orig_metrics = evaluate_all(orig_results, queries_df, k=5)\n",
    "comp_metrics = evaluate_all(comp_results, queries_df, k=5)\n",
    "\n",
    "import pandas as pd\n",
    "df_metrics = pd.DataFrame({\n",
    "    'Metric': ['P@5', 'R@5', 'MRR', 'MAP'],\n",
    "    'Original': [orig_metrics['P@5'], orig_metrics['R@5'], orig_metrics['MRR'], orig_metrics['MAP']],\n",
    "    'Compressed': [comp_metrics['P@5'], comp_metrics['R@5'], comp_metrics['MRR'], comp_metrics['MAP']]\n",
    "})\n",
    "df_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
