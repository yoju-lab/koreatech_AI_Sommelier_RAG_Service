# OpenAI 임베딩 모델과 Pinecone 인덱스 구축

## 세션 소개

한국어 영화 관련 데이터를 사용하여 **Pinecone 벡터 인덱스**를 만드는 방법을 배워보겠습니다. 이를 위해 OpenAI의 최신 임베딩 모델인 `text-embedding-3-small`을 활용하여 영화 텍스트 데이터를 벡터로 변환한 뒤 Pinecone에 저장합니다. 이번 세션에서는 **Pinecone 인덱스의 개념**, **Pinecone Python 클라이언트를 통한 인덱스 생성 방법**, **영화 메타데이터 구성 방법(장르, 감독, 년도, 평점 등)**, **벡터 유사도 검색과 키워드 필터링을 결합한 하이브리드 검색을 지원하는 필드 설계**, **텍스트와 메타데이터 및 임베딩을 Pinecone에 저장하는 방법** 등을 다룹니다. 또한 실습을 위해 사용할 수 있는 **한국어 영화 데이터셋**을 소개하고, 코드 예제와 Colab 노트북 형태로 따라할 수 있는 스크립트를 제공합니다.

> **Note:** 이번 세션에서는 **벡터 인덱스 구축과 메타데이터 설계**에 집중하며, 벡터 검색이나 쿼리 예시는 다루지 않습니다 (벡터 검색은 다음 세션에서 다룰 예정입니다).

## 한국어 영화 데이터셋 준비

실습에 사용할 예시 데이터셋으로 **한국 드라마/영화 메타데이터**를 활용하겠습니다. 예를 들어 Kaggle에 공개된 **“Korean Dramas Dataset”** (350개 작품, 2003\~2025년)이나 **“Ultimate Korean Drama List”** 등을 사용할 수 있습니다. 이러한 데이터셋에는 작품의 제목, 개봉년도, **장르**, **평점**, **출연 배우**, **에피소드 수** 등이 포함되어 있습니다. (예: *Drama Name*, *Year of Release*, *Watch Time*, *Drama Rating*, *Genre*, *Votes*, *Actors* 등의 열을 포함).

* **데이터셋 다운로드:** Kaggle에서 해당 데이터셋을 다운로드하거나, [DeepDataLake](https://deepdatalake.com)와 같은 사이트를 통해 CSV 파일을 얻을 수 있습니다 (예시: **KoreanDramas.csv**). 이 세션에서는 데이터셋의 예시를 보여주지만, Colab에서 실습할 때는 직접 해당 CSV 파일을 준비해 주세요.

* **데이터셋 내용:** 각 행은 한 편의 한국 드라마(또는 영화)를 나타내며, 작품명, 연도, 장르, 평균 평점, 투표 수, 주요 배우 목록 등의 메타데이터가 기록되어 있습니다. 다만 **줄거리 요약**(synopsis)은 포함되어 있지 않을 수 있습니다. 임베딩 모델을 활용한 벡터화에는 텍스트가 필요하므로, **별도의 줄거리 요약문**을 수집하거나 간략히 작성하여 사용하길 권장합니다. 예를 들어 각 작품에 대해 한국어로 된 한두 문장의 줄거리 소개를 메타데이터에 추가하면 이후의 임베딩 및 검색 품질을 높일 수 있습니다.

## Pinecone 인덱스 개요

Pinecone은 대규모 **벡터 데이터베이스**로, 임베딩 벡터들을 효율적으로 저장하고 유사도 검색을 수행할 수 있는 **인덱스(index)** 단위를 제공합니다. **Pinecone 인덱스**는 일종의 벡터 컬렉션으로서, 각각의 벡터 데이터에는 **고유 ID**, **벡터 임베딩**, 그리고 부가적인 **메타데이터**를 포함할 수 있습니다. 다시 말해, \*\*각 레코드(record)\*\*는 (ID, 벡터, 메타데이터)의 구조를 가지며, ID로 개별 벡터를 식별하고, 벡터는 텍스트 등의 의미를 수치화한 임베딩입니다. 메타데이터는 JSON 형태의 키-값 쌍으로 추가 정보를 저장할 수 있으며, 문자열, 숫자, 불리언, 혹은 문자열 리스트 등의 형식을 가질 수 있습니다.

Pinecone 인덱스는 내부적으로 대용량 벡터들을 분산 저장하고 **최근접 이웃 검색**(ANN)을 통해 주어진 쿼리 벡터와 가장 유사한 항목을 빠르게 찾아줍니다. 사용자는 **백엔드 인프라**(샤딩, 복제, 최적화 등)를 신경쓰지 않고 API를 통해 인덱스를 생성하고 벡터를 삽입/검색하면 됩니다. Pinecone가 인덱스를 생성하면, 해당 인덱스는 선택된 클라우드 리전에 할당되고, 지정된 metric (코사인 유사도 등)에 따라 벡터 유사도를 계산하도록 구성됩니다.

요약하면, Pinecone 인덱스는 우리 데이터(영화 줄거리 임베딩 벡터들)를 저장하고 관리하는 **컨테이너** 역할을 하며, **ID로 데이터 관리**, **벡터 유사도 검색**, **메타데이터 필터링** 등의 기능을 제공합니다.

## Pinecone Python 클라이언트를 활용한 인덱스 생성

이제 Python SDK를 사용하여 Pinecone에 접속하고 새로운 인덱스를 만들어보겠습니다. 먼저 Colab 환경 등에서 Pinecone 클라이언트와 OpenAI 패키지를 설치하고 API 키를 설정해야 합니다:

```python
!pip install -qU pinecone openai
import openai
from pinecone import Pinecone, ServerlessSpec

# API 키 설정 (본인의 API 키로 교체하세요)
openai.api_key = "YOUR_OPENAI_API_KEY"           # OpenAI API 키 입력
pc = Pinecone(api_key="YOUR_PINECONE_API_KEY")   # Pinecone API 키 입력
```

* **OpenAI API 키**는 OpenAI 플랫폼에서 발급받을 수 있고, **Pinecone API 키**는 Pinecone 콘솔에서 생성합니다. 두 키 모두 문자열로 코드에 제공하거나, 안전하게 다루려면 환경변수 등을 통해 불러올 수도 있습니다.

설치 및 인증이 완료되었다면, Pinecone에 \*\*인덱스(index)\*\*를 생성합니다. 인덱스를 생성할 때는 다음과 같은 주요 설정이 필요합니다:

* **인덱스 이름**: 고유한 이름을 지정합니다. (예: `"movie-index"`)
* **차원 수 (dimension)**: 벡터의 차원 크기로, 사용하려는 임베딩 모델의 출력 벡터 크기와 정확히 일치해야 합니다. 예를 들어 OpenAI의 `text-embedding-3-small` 모델의 벡터 차원은 **1536**입니다. 따라서 인덱스도 dimension=1536으로 생성해야 합니다.
* **유사도 매트릭(metric)**: 벡터 간 유사도 척도로, `"cosine"`(코사인 유사도), `"dotproduct"`(내적), `"euclidean"`(L2 거리) 중 선택합니다. 일반적으로 OpenAI 임베딩의 경우 코사인 유사도를 많이 사용하며, dotproduct도 자주 사용됩니다 (두 경우 큰 값 순으로 유사도가 높다고 판단). 이번 실습에서는 `"cosine"`을 사용하겠습니다.
* **클라우드 및 리전 설정**: Pinecone는 여러 리전에 걸쳐 서비스되므로, 인덱스를 어느 리전에 생성할지 지정해야 합니다. 무료/서버리스(Serverless) 플랜의 경우 제공되는 리전 예시로 `cloud="aws", region="us-east-1"` 등을 사용할 수 있습니다. (본인의 Pinecone 프로젝트에 맞는 리전을 선택하세요.)

다음은 Pinecone 인덱스를 생성하는 코드 예시입니다:

```python
# Pinecone 인덱스 생성
index_name = "movie-index"  # 인덱스 이름 설정
if not pc.has_index(index_name):
    pc.create_index(
        name=index_name,
        dimension=1536,            # 임베딩 벡터 차원 (모델 output 크기와 맞춤)
        metric="cosine",           # 유사도 metric 선택
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )
# 생성된 인덱스에 연결
index = pc.Index(index_name)
```

위 코드에서는 지정한 이름의 인덱스가 존재하지 않으면 `pc.create_index()`로 새 인덱스를 생성합니다. `ServerlessSpec`을 사용하여 AWS us-east-1 리전에 서버리스 인덱스를 만들고 있습니다. 서버리스 인덱스는 **Pod 용량을 신경쓸 필요 없이** 쉽게 시작할 수 있는 Pinecone의 관리형 인덱스입니다.

> 💡 **참고:** Pinecone에는 **서버리스(serverless) 인덱스**와 **Pod 기반 인덱스** 두 가지 운영 모드가 있습니다. 서버리스 인덱스는 소규모/개발 용도로 편리하며, 메타데이터 필드를 자동으로 모두 색인(index)합니다. Pod 기반 인덱스는 고성능/대규모 용도로 사용하며, 필요에 따라 메타데이터 색인을 선택적으로 설정할 수 있습니다 (고유값이 매우 많은 필드는 색인하지 않아 메모리를 절약 가능). 이번 실습에서는 서버리스 모드를 사용합니다.

인덱스 생성 후 `pc.Index(index_name)`를 통해 해당 인덱스 객체에 연결합니다. 이제 이 인덱스에 데이터를 입력(upsert)하고 활용할 준비가 되었습니다.

## 영화 데이터 메타데이터 구조화

Pinecone에 벡터를 올릴 때, 함께 저장할 \*\*메타데이터(metadata)\*\*를 잘 구조화하는 것이 중요합니다. 메타데이터는 각 벡터의 부가정보로 저장되며, 이후 검색 시 \*\*필터(filter)\*\*로 활용할 수 있습니다. 이번 영화 데이터 예시에서는 다음과 같은 메타데이터 필드를 설계할 수 있습니다:

* **title (제목)** – 영화 또는 드라마 제목 (문자열)
* **year (개봉년도)** – 작품의 연도 (정수)
* **genre (장르)** – 작품의 주요 장르 (문자열 혹은 문자열의 리스트)
* **director (감독)** – 감독 이름 (문자열)
* **actors (출연 배우)** – 주요 배우들의 이름 (여러 명일 경우 문자열 리스트)
* **rating (평점)** – 작품의 평점 (실수 또는 정수)
* **synopsis (줄거리)** – 작품의 간략한 줄거리 설명 (문자열, 한국어)

위의 필드 중 `synopsis`는 검색을 위한 **본문 텍스트**이며, 나머지 필드는 작품을 속성별로 필터링하거나 결과를 보여줄 때 활용합니다. Pinecone 메타데이터는 JSON 형태로 키-값 쌍을 저장하며, 값 타입으로 **문자열**, **숫자**(정수/부동소수), **불리언**, **문자열 리스트**가 지원됩니다.

예를 들어, 하나의 영화 메타데이터를 JSON으로 표현하면 다음과 같습니다:

```json
{
    "title": "기생충",
    "year": 2019,
    "genre": ["드라마", "스릴러"], 
    "director": "봉준호",
    "actors": ["송강호", "이선균", "조여정"],
    "rating": 8.6,
    "synopsis": "가난한 가족과 부유한 가족 사이에 벌어지는 예기치 않은 사건을 그린 블랙코미디 스릴러."
}
```

위 예시는 영화 \*\*「기생충」\*\*의 메타데이터를 나타낸 것으로, genre와 actors는 복수 값을 리스트로 갖고 있고, year는 정수, rating은 실수, 나머지는 문자열입니다. 이러한 구조화된 메타데이터를 사용하면, 예를 들어 사용자가 \*\*“2015년 이후 개봉한 스릴러 장르 영화”\*\*만 검색하도록 필터를 걸거나, **특정 배우가 출연한 영화만** 결과로 보이도록 제한할 수 있습니다.

메타데이터를 설계할 때는 **데이터 형식과 필드 이름**을 일관성 있게 유지하고, 이후 **검색에 활용할 필드**들을 포함시키는 것이 중요합니다. 불필요한 정보는 메타데이터에 넣지 않거나, 나중에 필터로 사용할 계획이 없다면 굳이 모두 저장할 필요는 없습니다. (예컨대, **서버리스 인덱스**에선 모든 필드가 필터 가능하지만, **Pod 인덱스**의 경우 너무 다양하고 고유한 값이 많은 필드를 색인하면 메모리 효율이 떨어질 수 있으므로 그런 필드는 메타데이터에 넣지 않거나 색인 대상에서 제외하는 것이 권장됩니다.)

또한 **메타데이터 크기 제한**은 벡터당 40KB이므로, 줄거리와 같이 비교적 긴 텍스트를 넣어도 대부분 문제없지만, 아주 긴 전문을 모두 넣는 것은 피하는 것이 좋습니다. 보통 줄거리는 수백자\~몇천자 정도로 요약하고, 필요하면 원문 데이터베이스에 ID로 연결해 두는 방식도 고려할 수 있습니다.

## 하이브리드 검색을 지원하는 필드 설계

**하이브리드 검색**이란 벡터 유사도 검색과 **속성/키워드 기반의 필터링**을 결합한 검색 방식을 말합니다. 예를 들어, 사용자가 자연어로 \*“SF 요소가 있는 한국 드라마 추천”\*이라고 질의하면, 단순 벡터 유사도만으로는 한국 드라마가 아닌 해외 작품이 결과에 섞일 수 있습니다. 하지만 **메타데이터 필드**를 잘 활용하면 \*“genre에 ‘SF’가 포함되고 country가 ‘Korea’인 데이터 내에서 벡터 유사도 top 결과”\*와 같이 **의미적 유사도**와 **키워드/속성 조건**을 모두 만족하는 결과를 얻을 수 있습니다.

이를 위해서는 **메타데이터 필드 설계** 단계에서부터 어떤 조건으로 필터링할지 염두에 두어야 합니다. 이번 영화 데이터 예시에서는 다음과 같은 하이브리드 검색 시나리오를 고려할 수 있습니다:

* **장르 필터링**: `genre` 필드를 이용해 특정 장르(예: 코미디, 다큐멘터리 등)만 검색하거나 제외할 수 있습니다. 예: `{"genre": {"$eq": "스릴러"}}` 또는 `{"genre": {"$in": ["드라마", "멜로"]}}` 같은 필터를 사용. 장르가 여러 개일 경우 문자열 리스트로 저장하여, `$in` 연산자를 통해 포함 여부를 검사합니다.
* **년도 필터링**: `year` 필드를 이용해 특정 연도 범위로 제한할 수 있습니다. Pinecone의 메타데이터 필터는 MongoDB 질의 문법을 부분적으로 지원하며, `$gte`, `$lte` 등의 연산자를 제공하므로 년도 >= 2020인 조건은 `{"year": {"$gte": 2020}}`처럼 표현합니다. 이를 활용해 2000년대 이후 작품만 검색하거나, 특정 년도 사이의 작품만 대상으로 할 수 있습니다.
* **평점 필터링**: `rating` 필드를 이용하면 평점이 특정 값 이상인 작품만 고르는 식의 조건이 가능합니다. 예: `{"rating": {"$gt": 8.0}}` (평점 8.0 초과). 평점은 소수점까지 있는 경우 부동소수 메타데이터로 저장됩니다.
* **인물/키워드 필터링**: `director`나 `actors` 필드를 이용해 특정 감독의 작품만, 혹은 특정 배우가 출연한 작품만 걸러낼 수 있습니다. 예: `{"actors": {"$in": ["송강호"]}}`라고 하면 송강호 배우가 포함된 작품만 검색 대상이 됩니다. 마찬가지로 여러 배우 동시 필터나, 감독+장르처럼 여러 필드를 조합한 AND 조건도 가능합니다 (`$and` 사용).
* **기타 맞춤 필드**: 만약 데이터에 **키워드**나 **태그** 목록이 있다면 (예: ‘복수극’, ‘타임슬립’ 등의 태그), 이를 메타데이터에 포함해 두면 특정 키워드가 달린 작품만 필터링할 수도 있습니다. 다만 이러한 자유형 텍스트 키워드는 필터보다는 임베딩의 의미 검색으로 처리하는 편이 일반적입니다. (좀 더 고급 주제로, \*\*희소 벡터(sparse vector)\*\*를 함께 저장하여 실제 키워드 매칭 점수를 병합하는 방식도 있는데, 이는 이번 범위에서는 다루지 않습니다.)

**실용 팁:** 필터로 사용할 일이 없을 필드는 메타데이터에 굳이 색인할 필요가 없습니다. Pinecone **서버리스 인덱스**에서는 모든 필드가 자동 색인되지만, **Pod 인덱스**에서는 `metadata_config` 옵션으로 꼭 필요한 필드만 색인함으로써 고유값이 많은 필드로 인한 메모리 낭비를 막을 수 있습니다. 예를 들어 각 항목마다 고유한 설명 텍스트(`synopsis`)는 필터링 조건으로 사용하지 않을 것이므로 Pod 인덱스라면 색인 대상에서 제외해도 됩니다. 반대로 장르, 연도처럼 자주 필터링하는 필드는 반드시 색인에 포함시켜야 합니다.

정리하면, 하이브리드 검색을 고려한 스키마 설계에서는 \*\*“어떤 필드로 검색 조건을 걸 것인가?”\*\*를 미리 생각하여 그 필드를 메타데이터로 저장하고 올바른 타입으로 지정해야 합니다. 이렇게 준비된 메타데이터와 의미 벡터를 함께 활용하면, 다음 세션에서 다룰 **벡터 쿼리 + 메타데이터 필터** 조합으로 강력한 검색을 구현할 수 있습니다.

## 텍스트 + 메타데이터 + 임베딩의 Pinecone 저장

마지막으로, 영화 데이터의 텍스트와 메타데이터를 임베딩 벡터로 변환하여 Pinecone 인덱스에 저장(upsert)해 보겠습니다. 크게 다음 **3단계**로 이루어집니다:

1. **텍스트 임베딩 생성:** OpenAI의 `text-embedding-3-small` 모델을 사용하여 각 영화의 줄거리 텍스트(`synopsis`)를 벡터로 변환합니다. 이 모델은 멀티링구얼 성능이 우수하여 한국어 텍스트도 잘 임베딩하며, 1536차원의 벡터를 반환합니다. OpenAI API를 호출하여 한번에 여러 문장을 배치로 임베딩할 수 있습니다.
2. **메타데이터 구성:** 각 영화의 메타데이터를 앞서 설계한 대로 딕셔너리(JSON) 형태로 정리합니다. 여기에는 제목, 년도, 장르, 평점, 감독, 배우 목록, 그리고 **텍스트 자체도 저장**할 수 있습니다. (예를 들어 나중에 검색결과를 보여줄 때 줄거리나 제목을 사용하려면 그것을 메타데이터에 포함시켜 두는 것이 편리합니다. 실제로 Pinecone에 벡터를 업sert할 때 메타데이터에 원본 텍스트를 함께 넣어두는 것이 일반적입니다.)
3. **Pinecone 업서트(upsert):** 준비된 ID, 벡터, 메타데이터를 Pinecone 인덱스에 upsert합니다. upsert는 삽입/갱신 작업으로, 주어진 ID에 벡터와 메타데이터를 저장합니다. 한 번에 여러 벡터를 리스트로 묶어 batch upsert하는 것이 효율적입니다.

이 과정을 코드로 구현해 보겠습니다. 우선, **데이터프레임**에서 필요한 정보를 추출하여 임베딩할 텍스트와 메타데이터 리스트를 생성합니다. (실제 실습 시에는 Kaggle 데이터셋을 불러와 사용하세요.)

```python
import pandas as pd

# 예시를 위한 간단한 데이터프레임 생성 (실제로는 CSV를 로드)
# df = pd.read_csv("KoreanDramas.csv")  # 실제 데이터셋 사용 시
data = [
    {
        "title": "응답하라 1988",
        "year": 2015,
        "genre": ["드라마", "코미디"],
        "director": "신원호",
        "actors": ["혜리", "박보검", "류준열"],
        "rating": 9.2,
        "synopsis": "1988년 서울, 쌍문동 이웃들 사이의 우정과 가족애를 그린 드라마."
    },
    {
        "title": "기생충",
        "year": 2019,
        "genre": ["드라마", "스릴러"],
        "director": "봉준호",
        "actors": ["송강호", "이선균", "조여정"],
        "rating": 8.6,
        "synopsis": "가난한 가족과 부유한 가족 사이 벌어지는 블랙코미디 풍자의 스릴러 영화."
    }
]
df = pd.DataFrame(data)
df.head(2)  # 데이터 확인 (실제로는 전체 데이터 사용)
```

위 예시는 실제 데이터셋의 일부분을 가상으로 만든 것입니다. 실제로 `df = pd.read_csv(...)`로 Kaggle에서 받은 CSV를 읽어오면 유사한 필드가 있을 것입니다 (`Drama Name`, `Year`, `Genre` 등). 해당 컬럼명을 적절히 변경하거나 사용할 수 있도록 전처리합니다. 또한, 줄거리 `synopsis` 컬럼이 없다면 직접 추가해주어야 합니다 (본 예시에서는 직접 리스트로 데이터에 넣었습니다).

이제 OpenAI 임베딩 API를 호출하여 **줄거리 텍스트를 벡터화**하겠습니다. OpenAI의 Python 라이브러리 `openai.Embedding`을 사용하면 됩니다:

```python
# 임베딩 할 문장 리스트 준비 (각 아이템의 synopsis 텍스트)
texts = df["synopsis"].tolist()

# 임베딩 생성 (배치 호출)
response = openai.Embedding.create(
    input=texts,
    model="text-embedding-3-small"
)
embeddings = [item["embedding"] for item in response["data"]]
```

위 코드는 `texts` 리스트의 각 한국어 문장에 대해 임베딩 벡터를 얻어 `embeddings` 리스트에 저장합니다. 한 번 호출로 여러 문장을 임베딩할 수 있으므로, 데이터가 많다면 100개 정도씩 batch로 나누어 호출하는 것이 좋습니다. 임베딩 API가 반환한 결과에서 `"embedding"` 값을 추출하여 Python 리스트로 변환했습니다. 이제 `embeddings[i]`가 `texts[i]`에 대응하는 1536차원 벡터입니다.

이제 **벡터와 메타데이터를 Pinecone 인덱스에 업서트**합니다. 업서트할 데이터는 `(id, vector, metadata)` 형태로 준비합니다. ID는 문자열이어야 하며, 각 데이터마다 고유하게 정합니다 (예: "movie-0", "movie-1", ... 또는 고유 제목 등). 메타데이터는 각 행의 정보를 바탕으로 딕셔너리를 구성하되, **JSON 직렬화 가능한 값**만 포함해야 함을 유의합니다 (파이썬 `int`, `float`, `str`, `bool`, `list` of `str` 등 가능). Pandas 데이터프레임에서 바로 딕셔너리로 만들 때 `NaN`이나 `None`이 있다면 제거하거나 변환해야 합니다.

```python
# 업서트할 벡터 레코드 리스트 구성
records_to_upsert = []
for idx, row in df.iterrows():
    vec_id = f"movie-{idx}"  # 고유 ID 생성
    vec = embeddings[idx]    # 해당 텍스트의 임베딩 벡터
    # 메타데이터 구성 (DataFrame row를 딕셔너리로 변환)
    meta = {
        "title": row["title"],
        "year": int(row["year"]),
        "genre": row["genre"],        # 리스트 또는 문자열
        "director": row["director"],
        "actors": row["actors"],      # 리스트
        "rating": float(row["rating"]),
        "synopsis": row["synopsis"]   # 원문 텍스트 저장 (옵션)
    }
    records_to_upsert.append((vec_id, vec, meta))

# Pinecone 업서트 (한번에 여러 벡터 저장)
index.upsert(vectors=records_to_upsert)
```

위 코드에서는 `df.iterrows()`를 사용하여 각 행을 순회하며 ID, 벡터, 메타데이터를 튜플로 만들어 `records_to_upsert` 리스트에 누적합니다. 그런 다음 `index.upsert(vectors=records_to_upsert)`를 호출하여 Pinecone에 일괄 전송합니다. Pinecone Python SDK는 이 리스트를 받아 서버로 전송하고, 인덱스에 벡터들을 저장합니다. 이때 새로운 ID이면 삽입(insert), 이미 존재하는 ID이면 내용을 갱신(update)하게 됩니다.

업서트 이후 Pinecone 인덱스에 데이터가 잘 들어갔는지 확인해볼 수 있습니다. 간단하게는 인덱스 통계를 조회하는 `describe_index_stats()`를 호출하여 총 벡터 수나 메타데이터 필드 정보를 볼 수 있습니다:

```python
stats = index.describe_index_stats()
print(stats)
```

이 함수는 인덱스에 대한 요약 정보를 반환합니다. 예를 들어 `{"total_vector_count": 350, ...}`처럼 벡터 개수가 나오고, 메타데이터 필드별로 값 분포가 나올 수도 있습니다. 이를 통해 우리가 기대한 만큼의 데이터가 적재되었는지 검증할 수 있습니다.

> **메타데이터 저장 여부:** 위 업서트 예시에서 `synopsis` 텍스트를 메타데이터에 포함했는데, 이렇게 하면 나중에 검색 결과에서 해당 텍스트를 쉽게 가져올 수 있습니다. Pinecone의 쿼리 결과는 해당 벡터의 메타데이터를 포함하도록 옵션을 줄 수 있는데 (`include_metadata=True`), 이를 설정하면 예컨대 유사한 줄거리 벡터를 찾은 뒤 그 \*\*제목(title)\*\*이나 \*\*줄거리(synopsis)\*\*를 함께 얻어 사용자에게 보여줄 수 있습니다.

이상으로, **한국어 영화 데이터**를 **벡터 임베딩**하고, 부가 정보와 함께 **Pinecone 인덱스에 저장**하는 전체 과정을 완료했습니다. 최종적으로 Pinecone 인덱스에는 각 영화/드라마의 임베딩 벡터와 메타데이터(제목, 년도, 장르 등)가 저장되어 있습니다. 이렇게 구축된 인덱스를 활용하면, 이후에 사용자의 자연어 질의에 대해 임베딩을 만든 후 Pinecone에서 **유사도 검색**을 수행하고, 필요에 따라 **메타데이터 필터**를 적용함으로써 원하는 결과를 얻을 수 있게 됩니다. (이 부분은 다음 시간의 주제입니다.)

## 결론 및 권장 사항

이번 세션에서는 **Pinecone**를 활용하여 한국어 영화 데이터를 위한 벡터 인덱스를 설계하고 구축하는 방법을 살펴보았습니다. **요약하면**:

* Pinecone 인덱스는 벡터 및 메타데이터를 저장하는 단위로, 각 레코드는 ID+벡터+메타데이터로 구성됩니다.
* OpenAI의 `text-embedding-3-small` 모델은 1536차원 임베딩을 생성하며 한국어 텍스트에 대해서도 강력한 성능을 보입니다. 해당 차원에 맞춰 Pinecone 인덱스를 생성하고 cosine 유사도 등을 설정했습니다.
* 영화 데이터의 중요한 속성들을 메타데이터로 저장하여, **하이브리드 검색**(벡터 유사도 + 속성 필터) 요구사항을 충족하도록 설계했습니다. 장르, 연도, 감독, 배우 등의 필드를 활용해 다양한 필터 조건(예: `{"year": {"$gte": 2020}}`, `{"genre": {"$in": ["코미디","드라마"]}}` 등)을 적용할 수 있습니다.
* Pinecone Python SDK를 통해 인덱스를 만들고, OpenAI API로 임베딩을 생성한 뒤, `index.upsert()`로 다수의 벡터를 일괄 삽입하는 방법을 실습했습니다. 또한 메타데이터에 원문 텍스트를 함께 저장하여 추후 결과 활용도를 높였습니다.
* **스키마 최적화** 측면에서, 필터링에 사용할 필드를 적절히 선택하고 데이터 타입을 맞춤으로써 검색 효율을 높일 수 있음을 강조했습니다. 또한 Pinecone 인덱스 모드에 따라 메타데이터 색인 전략을 달리할 수 있다는 점도 언급했습니다 (서버리스 인덱스는 모든 필드 자동 색인).

이제 이러한 벡터 인덱스가 구축되었으므로, 다음 단계로는 사용자의 질의를 받아 **벡터 검색**을 수행하고, 메타데이터 필터를 적용하여 적합한 한국 영화/드라마를 찾아내는 작업이 남아 있습니다. 다음 세션에서는 **Pinecone의 벡터 검색 쿼리와 필터 적용** 방법, 그리고 검색 결과를 해석하고 활용하는 방안을 다룰 예정입니다.

